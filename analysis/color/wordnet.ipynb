{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(l) :\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def nounify(adj_word):\n",
    "    \"\"\" Transform an adjective to the closest noun: dead -> death \"\"\"\n",
    "    adj_synsets = wn.synsets(adj_word, pos=\"a\")\n",
    "\n",
    "    # Word not found\n",
    "    if not adj_synsets:\n",
    "        return []\n",
    "\n",
    "    # Get all adj lemmas of the word\n",
    "    adj_lemmas = [l for s in adj_synsets \n",
    "                  for l in s.lemmas() \n",
    "                  if (s.name().split('.')[1] == 'a' or \n",
    "                      s.name().split('.')[1] == 's')]\n",
    "\n",
    "    # Get related forms\n",
    "    derivationally_related_forms = [(l, l.derivationally_related_forms()) \n",
    "                                    for l in adj_lemmas]\n",
    "\n",
    "    # filter only the nouns\n",
    "    related_noun_lemmas = [l for drf in derivationally_related_forms \n",
    "                           for l in drf[1] \n",
    "                           if l.synset().name().split('.')[1] == 'n']\n",
    "    synsets = [l.synset() for l in related_noun_lemmas]\n",
    "    return synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_human = (pd.read_csv('humanOutput/filteredCorpus.csv')\n",
    "     .assign(source = 'human'))\n",
    "d_prag = (pd.read_csv('modelOutput/speaker_big_sl_perp_sampled_message.csv')\n",
    "     .assign(source = 'pragmatic'))\n",
    "d_lit = (pd.read_csv('modelOutput/speaker_big_s0_untuned_sampled_message.csv')\n",
    "     .assign(source = 'literal'))\n",
    "d = d_human.append(d_prag).append(d_lit).replace(np.nan,' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: not actually safe to just take first element of `colorSynsets`: it's technically the most common one which is often what we want (e.g. 'red' for 'reddish') but the WordNet docs explicitly say that the order shouldn't be trusted in general. Not sure what we can do about this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def get_informativity(text):\n",
    "    try :\n",
    "        words = [wnl.lemmatize(word) for word in word_tokenize(text)]\n",
    "    except :\n",
    "        print(text)\n",
    "        raise\n",
    "    res = []\n",
    "    for word in words :\n",
    "        nounForms = wn.synsets(word, pos='n')\n",
    "        nounSynsets = nounForms if nounForms else nounify(word)\n",
    "        colorSynsets = [n for n in nounSynsets \n",
    "                        if 'color.n.01' in [s.name() for s in flatten(n.hypernym_paths())]]\n",
    "        res += [s.min_depth() for s in colorSynsets][:1] if colorSynsets else []\n",
    "    return np.max(res) if res else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dun.n.02'), Synset('fawn.n.02'), Synset('fawn.v.01'), Synset('fawn.v.02'), Synset('fawn.v.03')]\n",
      "9\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "text = \"fawn\"\n",
    "print(wn.synsets('fawn'))\n",
    "print(get_informativity(text))\n",
    "print(get_informativity('red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on whole dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d['specificity'] = [get_informativity(text) for text in d['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d.to_csv(\"informativities.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
