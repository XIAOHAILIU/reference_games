///fold:
var _powerset = function(set) {
  if (set.length == 0)
    return [[]];
  else {
    var rest = _powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

var powerset = function(set, opts) {
  var res = _powerset(set);
  return opts.noNull ? filter(function(x){return !_.isEmpty(x);}, res) : res;
};

var cartesianProductOf = function(listOfLists) {
  return reduce(function(b, a) { 
    return _.flatten(map(function(x) {     
      return map(function(y) {             
        return x.concat([y]);                   
      }, b);                                       
    }, a), true);                                  
  }, [ [] ], listOfLists);                                   
};

// these are the words in our vocabulary
var possibleWords = ['the', 'big', 'small', 'one', 'not'];

// Randomly delete a word from a list...
// var deleteWords = function(words) {
//   var wordToOmit = uniformDraw(words);
//   return remove(wordToOmit,words);
// };

// var insertWords = function(words) {
//   var insertLoc = randomInteger(words.length + 1);
//   var insertWord = uniformDraw(possibleWords);
//   return (words.slice(0,insertLoc)
//           .concat(insertWord)
//           .concat(words.slice(insertLoc, words.length)));
// };

// var replaceWords = function(words) {
//   var replaceLoc = randomInteger(words.length);
//   var replaceWord = uniformDraw(possibleWords);
//   return (words.slice(0,replaceLoc)
//           .concat(replaceWord)
//           .concat(words.slice(replaceLoc+1,words.length)));
// };

// It takes too long to explore the real space, so we'll use the toy
// version that doesn't allow repeats (e.g. only allowed to insert or
// replace words with new ones that aren't already in the list)
var deleteWords = function(words) {
  if(_.isEmpty(words)) {
    return words;
  } else {
    var wordToOmit = uniformDraw(words);
    return remove(wordToOmit,words);
  }
};

var insertWords = function(words) {
  if(_.isEmpty(_.difference(possibleWords, words))) {
    return words;
  } else {
    var insertLoc = randomInteger(words.length + 1);
    var insertWord = uniformDraw(_.difference(possibleWords, words));
    return (words.slice(0,insertLoc)
            .concat(insertWord)
            .concat(words.slice(insertLoc, words.length)));
  }
};

var replaceWords = function(words) {
  if(_.isEmpty(_.difference(possibleWords, words))) {
    return words;
  } else {
    
    var replaceLoc = randomInteger(words.length);
    var replaceWord = uniformDraw(_.difference(possibleWords, words));
    return (words.slice(0,replaceLoc)
            .concat(replaceWord)
            .concat(words.slice(replaceLoc+1,words.length)));
  }
};

var nullMeaning = function(x) {return true;};

var negate = function(f) {return function(x) {return !f(x)};}
var identity = function(x) {return x;};
var utteranceProbs = Infer({method: 'enumerate'}, function() {
  return normalize(repeat(5, function(){uniformDraw([.1, .5])}))
})
///

// possible states of the world
var states = ['t1', 't2'];
var statePrior = Categorical({vs: states, ps: [1/2, 1/2]});

// possible utterances (include null utterance to make sure dists are well-formed)
var unconstrainedUtterances = ['the big one', 'the small one'];
var derivedUtterances = ['the not big one', 'the not small one', 'n0'];
var utterances = unconstrainedUtterances.concat(derivedUtterances);
var utterancePrior = Categorical({vs: utterances, ps: [1/5,1/5,1/5,1/5,1/5]});

// longer utterances more costly
var uttCost = cache(function(utt) {
  return utt == 'n0' ? 10 : utt.split(' ').length;
});

// meanings are possible disjunctions of states 
var meanings = map(function(l){return l.join('|');}, 
                   powerset(states, {noNull: true}));
var meaningSets = cartesianProductOf(repeat(unconstrainedUtterances.length, 
                                            function() {return meanings;}));

// Lexicons are maps from utterances to meanings 
// (null utterance always goes to null meaning; negatives are derived)
var lexicons = map(function(meaningSet) {
  var unconstrainedMeanings = _.object(unconstrainedUtterances, meaningSet);
  var negations = _.object(map(function(v) {
    return [v, '!' + unconstrainedMeanings[remove('not', v.split(' ')).join(' ')]]
  }, derivedUtterances));
  return _.extend(unconstrainedMeanings, 
                  _.extend(negations, {'n0': 'null'}));
}, meaningSets);
var lexiconPrior = Categorical({vs: lexicons, ps: repeat(lexicons.length, function(){return 1/lexicons.length})});

// Looks up the meaning of an utterance in a lexicon object
var meaning = cache(function(utt, lexicon) {  
  var isNegative = lexicon[utt].slice(0,1) === '!'
  var label = isNegative ? lexicon[utt].slice(1) : lexicon[utt];
  var anyMeaning = function(trueState) {
    return any(function(labelState){
      return labelState == trueState;
    },label.split('|'));
  };
  return (label == 'null' ? nullMeaning : 
          isNegative ? negate(anyMeaning) : 
          anyMeaning);
});

// set speaker optimality & noiseRate
var params = {
  alpha : 2.5,
  noiseRate : .1,
  maxDepth : 3
};

// A transform is some sequence of corruptions applied to a word
// Not that this will end up giving more prior probability to sequences
// that can be formed in multiple ways (e.g. by adding and then deleting)
var transform = function(words, depth) {
  var operations = [deleteWords, insertWords, replaceWords, identity];
  var opSeq = repeat(depth, function(){return uniformDraw(operations);});
  return reduce(function(currOp, memo) {
    return currOp(memo);
  }, words, opSeq);
};

// Gives distribution over possible noisy versions of utt
var noiseModel = cache(function(utt) {
  return Infer({method: 'enumerate'}, function() {
    return (utt == 'n0' ? 'n0' :
	    flip(1-params.noiseRate) ? utt :
	    transform(utt.split(' '), params.maxDepth).join(' '));
  });
});

// literal listener w/ noisy channel inference
var L0 = cache(function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);

    var uttMeaning = meaning(intendedUtt, lexicon);
    factor(uttMeaning(state) ?
	   noiseModel(intendedUtt).score(utt) :
	   -100);
    return state;
  });
});

// pragmatic speaker
var S1 = cache(function(state, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var intendedUtt = sample(utterancePrior);
    var noisyUtt = sample(noiseModel(intendedUtt));
    factor(params.alpha * (L0(noisyUtt, lexicon).score(state)
			   - uttCost(noisyUtt)));
    return intendedUtt;
  });
});

// pragmatic listener (needed for S)
var L2 = cache(function(perceivedUtt, lexicon) {
  return Infer({method: 'enumerate'}, function() {
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);

    observe(noiseModel(intendedUtt), perceivedUtt);
    observe(S1(state, lexicon), intendedUtt);
    
    return state;
  });
});

// conventional listener
// Assumes:
// * all utterances, including current one, come from same lexicon
// * intended utt for each past data point is independent
// * speaker is S1 using utt to noisily communicate about some state 
var L = cache(function(perceivedUtt, data) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    var lexicon = sample(lexiconPrior);
    var intendedUtt = sample(utterancePrior);
    
    observe(noiseModel(intendedUtt), perceivedUtt);
    observe(S1(state, lexicon), intendedUtt);
    
    mapData({data: data}, function(datum){
      var intendedUtt = sample(utterancePrior);
      observe(noiseModel(intendedUtt), datum.utt);
      observe(S1(datum.obj, lexicon), intendedUtt);
    });
    return state;
  });
});

// conventional speaker
var S = function(state, data) {
  return Infer({method:"enumerate"}, function(){
    var lexicon = sample(lexiconPrior);
    var utteranceDist = sample(utteranceProbs);
    var intendedUtt = categorical({vs: utterances, ps: utteranceDist});
    var noisyUtt = sample(noiseModel(intendedUtt));

    factor(params.alpha * (L2(noisyUtt, lexicon).score(state)
                           - uttCost(noisyUtt)));
    mapData({data: data}, function(datum){
      var intendedUtt = categorical({vs: utterances, ps: utteranceDist});      
      observe(noiseModel(intendedUtt), datum.utt); // update beliefs about utterance dist
      observe(L2(datum.utt, lexicon), datum.obj); // update beliefs about lexicon
    });
    return noisyUtt;
  });
};

var speakerRes = function(state, data) {
  var res = S(state, data);
  console.log("the small one: " + Math.exp(res.score('the small one')));
  console.log("small: " + Math.exp(res.score('small')));
  console.log("ratio of 'the small one' to 'small'");
  console.log(Math.exp(res.score('the small one') - res.score('small')));
};

speakerRes('t1', []);
speakerRes('t1', [{utt: 'the small one', obj: 't1'}]);
speakerRes('t1', [{utt: 'the small one', obj: 't1'},
		  {utt: 'the big one', obj: 't2'}]);
speakerRes('t1', [{utt: 'the small one', obj: 't1'},
		  {utt: 'the big one', obj: 't2'},
		  {utt: 'the small one', obj: 't1'}]);

//speakerRes('t1', [{utt: 'small', obj: 't1'}]);
// speakerRes('t1', [{utt: 'the small one', obj: 't1'},
//                   {utt: 'the big one', obj: 't2'}]);
// speakerRes('t1', [{utt: 'the small one', obj: 't1'},
//                   {utt: 'the big one', obj: 't2'},
// 		  {utt: 'the small one', obj: 't1'}]);

// console.log(L('small', [{utt:'the small one', obj:'t1'}, {utt:'the big one', obj:'t2'}]));
// console.log(L('small', [{utt:'small', obj:'t1'},{utt:'big', obj:'t2'}]));
