///fold:
var _powerset = function(set) {
  if (set.length == 0)
    return [[]];
  else {
    var rest = _powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

var powerset = function(set, opts) {
  var res = _powerset(set);
  return opts.noNull ? filter(function(x){return !_.isEmpty(x);}, res) : res;
};

var cartesianProductOf = function(listOfLists) {
  return reduce(function(b, a) { 
    return _.flatten(map(function(x) {     
      return map(function(y) {             
        return x.concat([y]);                   
      }, b);                                       
    }, a), true);                                  
  }, [ [] ], listOfLists);                                   
};

var nullMeaning = function(x) {return true;};
///

// possible states of the world
var states = ['t1', 't2'];
var statePrior = Categorical({vs: states, ps: [1/2, 1/2]});

// possible utterances (include null utterance to make sure dists are well-formed)
var possibleWords = ['the', 'big', 'small', 'one', 'kinda'];
var utterances = ['the big one', 'the small one', 
                  'the kinda small one', 'the kinda big one', 'n0'];
var utterancePrior = Categorical({vs: utterances, ps: [1/5,1/5,1/5,1/5,1/5]});

// longer utterances more costly
var uttCost = cache(function(utt) {
  return utt == 'n0' ? 10 : utt.split(' ').length;
});

// meanings are possible disjunctions of states 
var meanings = map(function(l){return l.join('|');}, powerset(states, {noNull: true}));
var meaningSets = cartesianProductOf(repeat(utterances.slice(0,-1).length, function() {return meanings;}));

// Lexicons are maps from utterances to meanings (null utterance always goes to null meaning)
var lexicons = map(function(meaningSet) {
  return _.extend(_.object(utterances.slice(0,-1), meaningSet),
		  {'n0': 'null'});
}, meaningSets);

var lexiconPrior = Categorical({vs: lexicons, ps: repeat(lexicons.length, function(){return 1/lexicons.length})});

// Looks up the meaning of an utterance in a lexicon object
var meaning = cache(function(utt, lexicon) {
  var label = lexicon[utt];
  var anyMeaning = function(trueState) {
    return any(function(labelState){
      return labelState == trueState;
    },label.split('|'));
  };
  return label == 'null' ? nullMeaning : anyMeaning;
});

// set speaker optimality & noiseRate
var alpha = 5;
var noiseRate = .01;
var maxLevDistance = 0;

// Recursively delete words from an utterance with some rate
var deleteWords = function(words, levDistance) {
  if(_.isEmpty(words)) {
    return ['n0'];
  } else if(levDistance > maxLevDistance) {
    return words;
  } else {
    var wordToOmit = words[randomInteger(words.length)];
    var newWords = remove(wordToOmit,words);
    return (flip(noiseRate) ?
	    deleteWords(newWords, levDistance + 1) :
	    words);
  }
};

var insertWords = function(words, levDistance) {
  if(levDistance > maxLevDistance) {
    return words;
  } else {
    var insertLoc = randomInteger(words.length + 1);
    var insertWord = uniformDraw(possibleWords);
    var newWords = (words.slice(0,insertLoc)
		    .concat(insertWord)
		    .concat(words.slice(insertLoc, words.length)));
    return (flip(noiseRate) ?
	    insertWords(newWords, levDistance + 1) :
	    words);
  }
};

var replaceWords = function(words, levDistance) {
  if(levDistance > maxLevDistance) {
    return words;
  } else {
    var replaceLoc = randomInteger(words.length);
    var replaceWord = uniformDraw(possibleWords);
    var newWords = (words.slice(0,replaceLoc)
		    .concat(replaceWord)
		    .concat(words.slice(replaceLoc+1,words.length)));
    return (flip(noiseRate) ?
	    replaceWords(newWords, levDistance + 1) :
	    words);
  }
};

// Gives distribution over possible noisy versions of utt
var noiseModel = cache(function(utt) {
  return Infer({method: 'enumerate'}, function() {
    var words = utt.split(' ');
    var operations = [deleteWords, insertWords, replaceWords];
    var operation = uniformDraw(operations);
    return utt == 'n0' ? 'n0' : operation(words, 0).join(' ');
  });
});

var noisyUtterancesPrior = Infer({method: 'enumerate'}, function() {
  var intendedUtt = sample(utterancePrior);
  var noisyUtt = sample(noiseModel(intendedUtt));
  return noisyUtt;
});

// literal listener w/ noisy channel inference
var L0 = cache(function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);
    var uttMeaning = meaning(intendedUtt, lexicon);
    condition(uttMeaning(state));
    observe(noiseModel(intendedUtt), utt);
    return state;
  });
});

// pragmatic speaker
var S1 = cache(function(state, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var intendedUtt = sample(utterancePrior);
    var noisyUtt = sample(noiseModel(intendedUtt));
    factor(alpha * (L0(noisyUtt, lexicon).score(state)
		    - uttCost(noisyUtt)));
    return intendedUtt;
  });
});

var listenerObsModel = function(state, lexicon, iUtt, pUtt) {
  observe(noiseModel(iUtt), pUtt);
  observe(S1(state, lexicon), iUtt);
};

// pragmatic listener (needed for S)
var L2 = cache(function(utt, lexicon, data) {
  return Infer({method: 'enumerate'}, function() {
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);
    listenerObsModel(state, lexicon, intendedUtt, utt);
    return state;
  });
});

// conventional listener
// var L = function(utt, data) {
//   return Infer({method:"enumerate"}, function(){
//     var state = sample(statePrior);
//     var lexicon = sample(lexiconPrior);
//     var intendedUtt = sample(utterancePrior);
//     listenerObsModel(state, lexicon, intendedUtt, utt);
//     mapData({data: data}, function(datum){
//       var intendedUtt = sample(utterancePrior);
//       listenerObsModel(datum.obj, lexicon, intendedUtt, datum.utt);
//     });
//     return state;
//   });
// };

// conventional speaker
var S = function(state, data) {
  return Infer({method:"enumerate"}, function(){
    var lexicon = sample(lexiconPrior);
    var noisyUtt = sample(noisyUtterancesPrior);
    factor(alpha * (L2(noisyUtt, lexicon).score(state)
                    - uttCost(noisyUtt)));
    mapData({data: data}, function(datum){
      observe(noisyUtterancesPrior, datum.utt); // update beliefs about utterance dist
      observe(L2(datum.utt, lexicon), datum.obj); // update beliefs about lexicon
    });
    return {noisyUtt: noisyUtt};
  });
};

viz.marginals(S('t1', [{utt: 'the small one', obj: 't1'}]));
viz.marginals(S('t1', [{utt: 'the small one', obj: 't1'},
                       {utt: 'the small one', obj: 't1'},
                       {utt: 'the big one', obj: 't2'}]));
// viz.marginals(S('t1', [{utt: 'the small one', obj: 't1'},{utt: 'the big one', obj: 't2'}]));

// console.log(L2('the big one', {'the big one' : 't1|t2', 'the small one' : 't2', 'the kinda big one' : 't2', 'the kinda small one': 't1', 'n0' : 'null'}));
// console.log("listener hearing small one after data:");
// console.log(L('small', [{utt: 'small', obj: 't2'}]).print());
// console.log(lexicons.length);
// map(function(lexicon) {
//   console.log(lexicon);
//   console.log(L2('the small one', lexicon));
//   console.log(L2('the big one', lexicon));  
// }, lexicons);
// console.log(L2('the small one', {'the big one' : 't1|t2', 'the small one' : 't2', 'n0' : 'null'}).print());
// console.log(L2('the big one', {'the big one' : 't1|t2', 'the small one' : 't2', 'n0' : 'null'}).print());

//console.log(S('t1', [{utt: 'the big one', obj: 't2'}, {utt: 'the small one', obj: 't1'}]).print());
// console.log(S('t1', []));

// console.log(S('t1', [{utt: 'small', obj: 't1'}]).print());
// console.log(S('t1', [{utt: 'the small one', obj: 't1'}]).print());

// console.log("listener hearing label2 after data:");
// console.log(L('label2', [{utt: 'label1', obj: 'tangram1'}]).print());
