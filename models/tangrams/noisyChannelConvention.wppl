///fold:
var _powerset = function(set) {
  if (set.length == 0)
    return [[]];
  else {
    var rest = _powerset(set.slice(1));
    return map(function(element) {
      return [set[0]].concat(element);
    }, rest).concat(rest);
  }
};

var powerset = function(set, opts) {
  var res = _powerset(set);
  return opts.noNull ? filter(function(x){return !_.isEmpty(x);}, res) : res;
};

var cartesianProductOf = function(listOfLists) {
  return reduce(function(b, a) { 
    return _.flatten(map(function(x) {     
      return map(function(y) {             
        return x.concat([y]);                   
      }, b);                                       
    }, a), true);                                  
  }, [ [] ], listOfLists);                                   
};

// these are the words in our vocabulary
var possibleWords = ['the', 'big', 'small', 'one', 'not'];

// Randomly delete a word from a list...
var deleteWords = function(words) {
  var wordToOmit = uniformDraw(words);
  return remove(wordToOmit,words);
};

var insertWords = function(words) {
  var insertLoc = randomInteger(words.length + 1);
  var insertWord = uniformDraw(possibleWords);
  return (words.slice(0,insertLoc)
          .concat(insertWord)
          .concat(words.slice(insertLoc, words.length)));
};

var replaceWords = function(words) {
  var replaceLoc = randomInteger(words.length);
  var replaceWord = uniformDraw(possibleWords);
  return (words.slice(0,replaceLoc)
          .concat(replaceWord)
          .concat(words.slice(replaceLoc+1,words.length)));
};

var nullMeaning = function(x) {return true;};

var negate = function(f) {
  return function(x) {return !f(x)};
}
var utteranceProbs = Infer({method: 'enumerate'}, function() {
  return normalize(repeat(5, function(){uniformDraw([.1, .5])}))
})
///

// possible states of the world
var states = ['t1', 't2'];
var statePrior = Categorical({vs: states, ps: [1/2, 1/2]});

// possible utterances (include null utterance to make sure dists are well-formed)
var unconstrainedUtterances = ['the big one', 'the small one'];
var derivedUtterances = ['the not big one', 'the not small one', 'n0'];
var utterances = unconstrainedUtterances.concat(derivedUtterances);
var utterancePrior = Categorical({vs: utterances, ps: [1/5,1/5,1/5,1/5,1/5]});

// longer utterances more costly
var uttCost = cache(function(utt) {
  return utt == 'n0' ? 10 : utt.split(' ').length;
});

// meanings are possible disjunctions of states 
var meanings = map(function(l){return l.join('|');}, 
                   powerset(states, {noNull: true}));
var meaningSets = cartesianProductOf(repeat(unconstrainedUtterances.length, 
                                            function() {return meanings;}));

// Lexicons are maps from utterances to meanings 
// (null utterance always goes to null meaning; negatives are derived)
var lexicons = map(function(meaningSet) {
  var unconstrainedMeanings = _.object(unconstrainedUtterances, meaningSet);
  var negations = _.object(map(function(v) {
    return [v, '!' + unconstrainedMeanings[remove('not', v.split(' ')).join(' ')]]
  }, derivedUtterances));
  return _.extend(unconstrainedMeanings, 
                  _.extend(negations, {'n0': 'null'}));
}, meaningSets);
var lexiconPrior = Categorical({vs: lexicons, ps: repeat(lexicons.length, function(){return 1/lexicons.length})});

// Looks up the meaning of an utterance in a lexicon object
var meaning = cache(function(utt, lexicon) {  
  var isNegative = lexicon[utt].slice(0,1) === '!'
  var label = isNegative ? lexicon[utt].slice(1) : lexicon[utt];
  var anyMeaning = function(trueState) {
    return any(function(labelState){
      return labelState == trueState;
    },label.split('|'));
  };
  return (label == 'null' ? nullMeaning : 
          isNegative ? negate(anyMeaning) : 
          anyMeaning);
});

// set speaker optimality & noiseRate
var params = {
  alpha : 100,
  noiseRate : .5,
  maxDepth : 2
};

var transform = function(words, depth) {
  if(_.isEmpty(words)) {
    return ['n0'];
  } else if(depth > params.maxDepth) {
    return words;
  } else {
    var operations = [deleteWords, insertWords, replaceWords];
    var operation = uniformDraw(operations);
    return flip(params.noiseRate) ? transform(operation(words), depth + 1) : words;
  }
}

// Gives distribution over possible noisy versions of utt
var noiseModel = cache(function(utt) {
  return Infer({method: 'enumerate'}, function() {
    var words = utt.split(' ');
    return utt == 'n0' ? 'n0' : transform(words, 0).join(' ');
  });
});

// literal listener w/ noisy channel inference
var L0 = cache(function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);
    var uttMeaning = meaning(intendedUtt, lexicon);
    factor(uttMeaning(state) ? 0 : -100);
    observe(noiseModel(intendedUtt), utt);
    return state;
  });
});

// pragmatic speaker
var S1 = cache(function(state, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var intendedUtt = sample(utterancePrior);
    var noisyUtt = sample(noiseModel(intendedUtt));
    factor(params.alpha * (L0(noisyUtt, lexicon).score(state)
                    - uttCost(noisyUtt)));
    return intendedUtt;
  });
});

var listenerObsModel = function(state, lexicon, iUtt, pUtt) {
  observe(noiseModel(iUtt), pUtt);
  observe(S1(state, lexicon), iUtt);
};

// pragmatic listener (needed for S)
var L2 = cache(function(utt, lexicon) {
  return Infer({method: 'enumerate'}, function() {
    var state = sample(statePrior);
    var intendedUtt = sample(utterancePrior);
    listenerObsModel(state, lexicon, intendedUtt, utt);
    return state;
  });
});

// conventional listener
var L = function(utt, data) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    var lexicon = sample(lexiconPrior);
     var intendedUtt = sample(utterancePrior)
//    var intendedUttProbs = sample(utteranceProbs);
//    var intendedUtt = categorical({vs: utterances, ps: intendedUttProbs})
    listenerObsModel(state, lexicon, intendedUtt, utt);
    mapData({data: data}, function(datum){
      var intendedUtt = sample(utterancePrior)
      listenerObsModel(datum.obj, lexicon, intendedUtt, datum.utt);
    });
    return state//, smallProb: intendedUttProbs[1], notSmallProb: intendedUttProbs[3]}
  });
};

// viz.marginals(L('big one', [{utt:'the big one', obj:'t1'}, {utt:'the big one', obj:'t1'}, {utt:'the big one', obj:'t1'}]))
// viz.marginals(L('big one', [{utt:'the small one', obj:'t1'}, {utt:'the small one', obj:'t1'}, {utt:'the small one', obj:'t1'}]))

// viz.marginals(L('small', []))
// viz.marginals(L('the small one', []))
// viz.table(L('small', [{utt:'the small one', obj:'t1'}, {utt:'the big one', obj:'t2'}]))
// viz.table(L('small', [{utt:'small', obj:'t1'},{utt:'big', obj:'t2'}]))

// conventional speaker
var S = function(state, data) {
  return Infer({method:"enumerate"}, function(){
    var lexicon = sample(lexiconPrior);
    var intendedUtt = sample(utterancePrior);
    var noisyUtt = sample(noiseModel(intendedUtt));

    print(noisyUtt);
    factor(params.alpha * (L2(noisyUtt, lexicon).score(state)
                           - uttCost(noisyUtt)));
    mapData({data: data}, function(datum){
      var intendedUtt = sample(utterancePrior);
      observe(noiseModel(intendedUtt), datum.utt); // update beliefs about utterance dist
      observe(L2(datum.utt, lexicon), datum.obj); // update beliefs about lexicon
    });
    return noisyUtt;
  });
};

// viz.table(S('t1', []));
// viz.table(S('t1', [{utt: 'the small one', obj: 't1'}]), {top: 10});
// viz.table(S('t1', [{utt: 'the small one', obj: 't1'},
//                    {utt: 'the small one', obj: 't1'}]), {top: 10});
// viz.table(S('t1', [{utt: 'the small one', obj: 't1'},{utt: 'the big one', obj: 't2'}]));
