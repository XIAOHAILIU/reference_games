---
title: "Arbitrariness and path dependence in a noisy-channel communication model"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: 
  \author{{\large \bf Robert Hawkins, Michael Frank, Noah Goodman} \\ \texttt{\{rxdh, mcfrank, ngoodman\}@stanford.edu} \\ Department of Psychology \\ Stanford University}
 
abstract: 
    ""
    
keywords:
    "conventions; pragmatics; communication"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(ggplot2)
# library(lme4)
# library(lmerTest)
library(tidyr)
library(dplyr)
library(qdap)
library(stringr)
library(knitr)
library(NLP)
library(readr)
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

# Introduction

Successful communication depends on a set of shared linguistic conventions: arbitrary but self-sustaining solutions to coordination problems [@Lewis69_Convention]. These conventions allow communities of speakers to coordinate group behavior (cite collective behavior lit??), initiate speech acts [@Strawson64_IntentionConvention], and align beliefs or memories [@StolkVerhagenToni16_ConceptualAlignment; @ComanEtAl16_MnemonicConvergence]. 
While *global* conventions adopted and sustained throughout a large population of speakers may form over much longer time scales (cite historical ling?), we also effortlessly coordinate on *ad hoc* or *local* conventions -- or conceptual pacts -- within the span of a single dialogue. Since global conventions emerge through repeated interactions of this kind [@GarrodDoherty94_GroupConventionsLinguistics], the cognitive mechanisms underlying conventionalization in dyadic games are of great interest.

For instance, in a seminal study by @ClarkWilkesGibbs86_ReferringCollaborative, pairs of participants played an interactive game where they were presented with arrays of abstract tangram shapes in randomized orders. One player -- the *speaker* -- was instructed to decribe the tangrams such that the other player -- the *listener* -- could rearrange their tangrams to accurately match the speaker's board. Over six rounds, descriptions were dramatically shortened: an early description like "All right, the next one looks like a person who’s ice skating, except they’re sticking two arms out in front" became "The ice skater" by the final round. This was taken as evidence of a collaborative process by which participants coordinate on a mutually understood referring expression, or convention, that minimizes joint effort. 

These results raised a number of theoretical questions that remain unresolved: To what extent are these conventions arbitrary? Why are some words from the initial utterance more likely to be dropped in future rounds than others? If conventions emerge from a process of social reasoning, how do agent represent one another, and how do they update those representations as the game progresses? In this paper, we first use modern NLP techniques to expand upon these early results in a large-scale replication of the tangrams task. 
Next, we show that a computational model of communication in repeated reference games, based on recent successes capturing language understanding as social inference [@GoodmanStuhlmuller13_KnowledgeImplicature;@GoodmanFrank16_RSATiCS], can account for key empirical signatures of conventionalization, including arbitrariness, path-dependence, and systematic shortening of utterances over time. 

\textcolor{red}{Need a theoretical summary of modeling contribution here. One possibility is to frame it in terms of the need for pragmatics \& social reasoning (vs. pure heuristic updating as in Barr, 2004) or in terms of our perspective on establishing common ground (i.e. that you don't need to represent the common ground as an infinite recursion or some separate object you both reason about, just need to assume that other player knows true meanings...)}

<!-- We argue that the initial seeds of conventions are *non-arbitrary* to a degree, in the sense that they are chosen proportional to their informativity in context. Yet subsequent usage is also *path-dependent* in the sense that different seeds persist across time within different groups. -->

# Large-scale tangrams replication

```{r}
# Import message data...
tangramMsgs = read_csv("../../analysis/tangrams/handTagged.csv") %>%
  rename(msgTime = time, 
         role = sender)

# Import survey data...
tangramSubjInfo = read.csv("../../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role'))

# Exclusion criteria
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 6))$gameid)
badGames <- union(incompleteIDs, nonNativeSpeakerIDs)

# filter & preprocess
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = word_count(contents, digit.remove = F)) %>%
  filter(!is.na(numRawWords)) %>% # filter out pure punctuation messages
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
```

## Participants

200 participants were recruited from Amazon's Mechanical Turk and paired into dyads to play a real-time communication game using the framework in @Hawkins15_RealTimeWebExperiments. We excluded \textcolor{red}{X} games because one or both of the participants reported being a native language different from English, and \textcolor{red}{Y} games for terminating early, leaving a corpus of `r numGames` complete games with a total of `r numUtterances` utterances.

## Stimuli

```{r image, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:taskScreenshot} Example trial"}
img <- png::readPNG("figs/directorBoard.png")
grid::grid.raster(img)
```

On every trial of the game, both participants were shown a grid of twelve tangram shapes, reproduced from @ClarkWilkesGibbs86_ReferringCollaborative. The cells were all labeled with numbers from one to twelve, as an aid in reference (see Fig. \ref{fig:taskScreenshot}).

## Procedure

After passing a short quiz about task instructions, participants were randomly assigned the role of either 'director' or 'matcher' and automatically paired into virtual rooms containing a chat box and grid of stimuli. Both participants could freely use the chat box to communicate at any time, but only the matcher could click and drag stimuli to reorder them. When the players were satisfied with their tangram arrangements, the matcher clicked a 'submit' button that gave players feedback on their score (out of 12) and scrambled the tangrams for the next round. After six rounds, players were redirected to a short exit survey. We collected the raw text of every message sent, and every swapping action taken by the matcher. 

## Results 

### Preprocessing 

```{r}
# Note that this includes Nicki's old data...
tagCounts = tangramMsgs %>% group_by(tangramRef) %>% tally() %>% spread(tangramRef, n)
pctTagged = 100*(1-(tagCounts$None/sum(tagCounts)))
```

Since the task was conducted with minimally constrained language use, we began by tagging which tangram was being referred to, if any, in each message. Instead of hand-tagging nearly 10,000 items, we used a hybrid strategy. First, since many of the director's messages explicitly referred to a cell number, we first cross-referenced these numbers with the locations of each tangram in their array. This successfully labelled 38.4% of utterances. Next, we combined this automatically tagged data with a previously hand-tagged pilot corpus containing 3955 utterances, and created a 80/20 split to train and evaluate a classifier that maps utterances to tangram tags. Each utterance string was vectorized into a matrix of unigram and bigram counts, then transformed into a normalized tf-idf representation (which weights counts by their overall frequency in the corpus). 

We trained our logistic classifier using stochastic gradient descent, yielding 75% accuracy on the test split. We then constructed an ROC curve examining the tradeoff between the true positive and false positive rate for different confidence thresholds, and selected a cautious threshold of 90% confidence that minimized the false positive rate to <5%. Finally, we used this threshold to label the subset of our full corpus where we are sufficiently confident. We iterated this process with batches of hand-tagging until `r round(pctTagged,2)`% of the corpus was assigned a tag.

### Reduction in raw length

First, we find that the mean number of words used by speakers decreases over time (see Fig. \ref{fig:replication}). This replicates a highly reliable reduction effect found throughout the literature on iterated reference games [@KraussWeinheimer64_ReferencePhrases;@BrennanClark96_ConceptualPactsConversation]. 

```{r replicationfig, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:replication} Reduction in the mean length of utterance over time. Error bars represent 1 SE."}
ggplot(tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         summarize(m = mean(individualM), 
                   se = sd(individualM)/sqrt(length(individualM))), 
       aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("mean number words (by director) per figure") +
  xlab("trials") +
  ylim(0,20) +
  xlim(0, 7) +
  theme_bw() 
```

## Part-of-Speech reduction

Next, we ran a part-of-speech tagger on all utterances and counted 

```{r POSfig, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:POS} Percent reduction between first and last round for closed-class and open-class parts of speech over time. Error bars represent 1 SE."}
posReduction = read.csv('../../analysis/tangrams/posTagged.csv', header =T) %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>% 
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>% 
  # Take mean & se over participants
  summarize(diffPctM = mean(diffPct),
            diffPctSE = sd(diffPct)/sqrt(length(diffPct))) %>%
  filter(POS != "OTHER") %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps', 'adverbs'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -diffPctM) )

ggplot(posReduction, aes(x = POS, y = diffPctM, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = diffPctM + diffPctSE, ymin = diffPctM - diffPctSE), width = .1)+
  theme_bw() +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

# Model

While simple evolutionary or agent-based models have previously been used to investigate the dynamics of *global* convention formation in signalling games [@ShohamTennenholtz97_EmergenceOfConventions;@Delgado02_ConventionsNetworks;@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems], relatively little modeling work has focused on the cognitive mechanisms supporting emergence of ad hoc conventions during shorter dyadic interactions. In fact, the systematic reduction in referring expressions within an interaction poses some problems for these classes of models. Evolutionary models 

Our probabilistic model makes use of two recent innovations in modeling language understanding using Rational Speech Act (RSA) models: lexical uncertainty [@BergenLevyGoodman16_LexicalUncertainty] and a noisy communication channel [@GibsonBergenPiantadosi13_RationalIntegrationNoisy;@BergenGoodman15_StrategicUseOfNoise]. The former introduces initial uncertainty over the exact meanings of words in the lexicon, such that agents jointly infer the meanings of utterances along with the information those utterances are being used to communicate. The latter introduces a perceptual noise model by which utterances can be corrupted during transmission; listeners invert this noise model to infer likely intended utterances from their noisy percepts, and speakers take this source of noise into account when selecting what utterance to use. 

We gradually build up to this combined model by tackling simpler sub-problems, beginning with a reimplementation of the pure lexical uncertainty model from @SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS. 

## Arbitrariness

A critical aspect of social conventions, and linguistic conventions in particular, is their arbitrariness: "cat" (English) and "chat" (French) are equally good ways of referring to a feline in their respective language communities. However, recent studies  emphasizing specific ways in which language is non-arbitrary [@MonaghanEtAl14_ArbitraryLanguage; @DingemanseEtAl15_IconicityLanguage; @LewisFrank16_LengthOfWordsComplexity; @BlasiEtAl16_SoundMeaningAssociation] force us to consider exactly in what sense arbitrariness is an essential property of conventions. Are all conventions created equal? 

<!-- # General Formatting Instructions  -->

<!-- For general information about authoring in markdown, see **[here](http://rmarkdown.rstudio.com/authoring_basics.html).** -->

<!-- For standard spoken papers and standard posters, the entire  -->
<!-- contribution (including figures, references, everything) can be  -->
<!-- no longer than six pages. For abstract posters, the entire contribution  -->
<!-- can be no longer than one page. For symposia, the entire contribution  -->
<!-- can be no longer than two pages. -->

<!-- The text of the paper should be formatted in two columns with an -->
<!-- overall width of 7 inches (17.8 cm) and length of 9.25 inches (23.5 -->
<!-- cm), with 0.25 inches between the columns. Leave two line spaces -->
<!-- between the last author listed and the text of the paper. The left -->
<!-- margin should be 0.75 inches and the top margin should be 1 inch. -->
<!-- \textbf{The right and bottom margins will depend on whether you use -->
<!-- U.S. letter or A4 paper, so you must be sure to measure the width of -->
<!-- the printed text.} Use 10 point Times Roman with 12 point vertical -->
<!-- spacing, unless otherwise specified. -->

<!-- The title should be in 14 point, bold, and centered. The title should -->
<!-- be formatted with initial caps (the first letter of content words -->
<!-- capitalized and the rest lower case). Each author's name should appear -->
<!-- on a separate line, 11 point bold, and centered, with the author's -->
<!-- email address in parentheses. Under each author's name list the -->
<!-- author's affiliation and postal address in ordinary 10 point  type. -->

<!-- Indent the first line of each paragraph by 1/8~inch (except for the -->
<!-- first paragraph of a new section). Do not add extra vertical space -->
<!-- between paragraphs. -->

<!-- # First-Level Headings -->

<!-- First level headings should be in 12 point , initial caps, bold and -->
<!-- centered. Leave one line space above the heading and 1/4~line space -->
<!-- below the heading. -->

<!-- ## Second-Level Headings -->

<!-- Second level headings should be 11 point , initial caps, bold, and -->
<!-- flush left. Leave one line space above the heading and 1/4~ line -->
<!-- space below the heading. -->

<!-- ### Third-Level Headings -->

<!-- Third-level headings should be 10 point , initial caps, bold, and flush -->
<!-- left. Leave one line space above the heading, but no space after the -->
<!-- heading. -->

<!-- # Formalities, Footnotes, and Floats -->

<!-- Use standard APA citation format. Citations within the text should -->
<!-- include the author's last name and year. If the authors' names are -->
<!-- included in the sentence, place only the year in parentheses, as in -->
<!-- [-@NewellSimon1972a], but otherwise place the entire reference in -->
<!-- parentheses with the authors and year separated by a comma -->
<!-- [@NewellSimon1972a]. List multiple references alphabetically and -->
<!-- separate them by semicolons [@ChalnickBillman1988a; @NewellSimon1972a].  -->
<!-- Use the et. al. construction only after listing all the authors to a -->
<!-- publication in an earlier reference and for citations with four or -->
<!-- more authors. -->

<!-- For more information on citations in R Markdown, see **[here](http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html#citations).** -->

<!-- ## Footnotes -->

<!-- Indicate footnotes with a number\footnote{Sample of the first -->
<!-- footnote.} in the text. Place the footnotes in 9 point type at the -->
<!-- bottom of the page on which they appear. Precede the footnote with a -->
<!-- horizontal rule.\footnote{Sample of the second footnote.} -->

<!-- ## Figures -->

<!-- All artwork must be very dark for purposes of reproduction and should -->
<!-- not be hand drawn. Number figures sequentially, placing the figure -->
<!-- number and caption, in 10 point, after the figure with one line space -->
<!-- above the caption and one line space below it. If necessary, leave extra white space at -->
<!-- the bottom of the page to avoid splitting the figure and figure -->
<!-- caption. You may float figures to the top or bottom of a column, or -->
<!-- set wide figures across both columns. -->

<!-- ## Two-column images -->

<!-- You can read local images using png package for example and plot  -->
<!-- it like a regular plot using grid.raster from the grid package.  -->
<!-- With this method you have full control of the size of your image. **Note: Image must be in .png file format for the readPNG function to work.** -->

<!-- You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`. -->

<!-- ```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."} -->
<!-- ``` -->

<!-- ## One-column images -->

<!-- Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`. -->

<!-- ```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{tab:tab2} Caption caption caption"} -->
<!-- ``` -->


<!-- ## R Plots -->

<!-- You can use R chunks directly to plot graphs. And you can use latex floats in the -->
<!-- fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)** -->

<!-- ```{r plot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=2, fig.height=2, fig.cap = "R plot" } -->
<!-- x <- 0:100 -->
<!-- y <- 2 * (x + rnorm(length(x), sd = 3) + 3) -->

<!-- ggplot2::ggplot(data = data.frame(x, y),  -->
<!--        aes(x = x, y = y)) +  -->
<!--   geom_point() +  -->
<!--   geom_smooth(method = "lm") -->
<!-- ``` -->


<!-- ## Tables -->

<!-- Number tables consecutively; place the table number and title (in -->
<!-- 10 point) above the table with one line space above the caption and -->
<!-- one line space below it, as in Table 1. You may float -->
<!-- tables to the top or bottom of a column, set wide tables across both -->
<!-- columns. -->

<!-- You can use the xtable function in the xtable package. -->

<!-- ```{r xtable, results="asis"} -->
<!-- n <- 100 -->
<!-- x <- rnorm(n) -->
<!-- y <- 2*x + rnorm(n) -->
<!-- out <- lm(y ~ x) -->

<!-- tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2),  -->
<!--                       caption = "This table prints across one column.") -->

<!-- print(tab1, type="latex", comment = F, table.placement = "H") -->
<!-- ``` -->

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
