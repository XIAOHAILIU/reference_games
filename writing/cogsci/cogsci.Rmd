---
title: "Arbitrariness and path dependence in a noisy-channel communication model"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: 
  \author{{\large \bf Robert Hawkins, Michael Frank, Noah Goodman} \\ \texttt{\{rxdh, mcfrank, ngoodman\}@stanford.edu} \\ Department of Psychology \\ Stanford University}
 
abstract: 
    ""
    
keywords:
    "conventions; pragmatics; communication"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(ggplot2)
library(lme4)
library(entropy)
library(tm)
library(tidyr)
library(dplyr)
library(qdap)
library(stringr)
library(knitr)
library(NLP)
library(readr)
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

# Introduction

Successful communication depends on a set of shared linguistic conventions: arbitrary but self-sustaining solutions to coordination problems [@Lewis69_Convention]. These conventions allow communities of speakers to coordinate group behavior (cite collective behavior lit??), initiate speech acts [@Strawson64_IntentionConvention], and align beliefs or memories [@StolkVerhagenToni16_ConceptualAlignment; @ComanEtAl16_MnemonicConvergence]. 
While *global* conventions adopted and sustained throughout a large population of speakers may form over much longer time scales (cite historical ling?), we also effortlessly coordinate on *ad hoc* or *local* conventions -- or conceptual pacts -- within the span of a single dialogue. Since global conventions emerge through repeated interactions of this kind [@GarrodDoherty94_GroupConventionsLinguistics], the cognitive mechanisms underlying conventionalization in dyadic games are of great interest.

For instance, in a seminal study by @ClarkWilkesGibbs86_ReferringCollaborative, pairs of participants played an interactive game where they were presented with arrays of abstract tangram shapes in randomized orders. One player -- the *speaker* -- was instructed to decribe the tangrams such that the other player -- the *listener* -- could rearrange their tangrams to accurately match the speaker's board. Over six rounds, descriptions were dramatically shortened: an early description like "All right, the next one looks like a person who’s ice skating, except they’re sticking two arms out in front" became "The ice skater" by the final round. This was taken as evidence of a collaborative process by which participants coordinate on a mutually understood referring expression, or convention, that minimizes joint effort. 

These results raised a number of theoretical questions that remain unresolved: To what extent are these conventions arbitrary? Why are some words from the initial utterance more likely to be dropped in future rounds than others? If conventions emerge from a process of social reasoning, how do agent represent one another, and how do they update those representations as the game progresses? In this paper, we first use modern NLP techniques to expand upon these early results in a large-scale replication of the tangrams task. 
Next, we show that a computational model of communication in repeated reference games, based on recent successes capturing language understanding as social inference [@GoodmanStuhlmuller13_KnowledgeImplicature;@GoodmanFrank16_RSATiCS], can account for key empirical signatures of conventionalization, including arbitrariness, path-dependence, and systematic shortening of utterances over time. 

\textcolor{red}{Need a theoretical summary of modeling contribution here. One possibility is to frame it in terms of the need for pragmatics \& social reasoning (vs. pure heuristic updating as in Barr, 2004) or in terms of our perspective on establishing common ground (i.e. that you don't need to represent the common ground as an infinite recursion or some separate object you both reason about, just need to assume that other player knows true meanings...)}

<!-- We argue that the initial seeds of conventions are *non-arbitrary* to a degree, in the sense that they are chosen proportional to their informativity in context. Yet subsequent usage is also *path-dependent* in the sense that different seeds persist across time within different groups. -->

# Large-scale tangrams replication

```{r}
# Import message data...
tangramMsgs = read_csv("../../analysis/tangrams/handTagged.csv") %>%
  rename(msgTime = time, 
         role = sender)

# Import survey data...
tangramSubjInfo = read.csv("../../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role'))

# Exclusion criteria
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 6))$gameid)
badGames <- union(incompleteIDs, nonNativeSpeakerIDs)

# filter & preprocess
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = word_count(contents, digit.remove = F)) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages
  #filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
```

## Participants

200 participants were recruited from Amazon's Mechanical Turk and paired into dyads to play a real-time communication game using the framework in @Hawkins15_RealTimeWebExperiments. We excluded \textcolor{red}{X} games because one or both of the participants reported being a native language different from English, and \textcolor{red}{Y} games for terminating early, leaving a corpus of `r numGames` complete games with a total of `r numUtterances` utterances. \textcolor{red}{rdh: this technically includes nicki's data, too; need to make this clearer}

## Stimuli

```{r image, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:taskScreenshot} Example trial"}
img <- png::readPNG("figs/directorBoard.png")
grid::grid.raster(img)
```

On every trial of the game, both participants were shown a grid of twelve tangram shapes, reproduced from @ClarkWilkesGibbs86_ReferringCollaborative. The cells were all labeled with numbers from one to twelve, as an aid in reference (see Fig. \ref{fig:taskScreenshot}).

## Procedure

After passing a short quiz about task instructions, participants were randomly assigned the role of either 'director' or 'matcher' and automatically paired into virtual rooms containing a chat box and grid of stimuli. Both participants could freely use the chat box to communicate at any time, but only the matcher could click and drag stimuli to reorder them. When the players were satisfied with their tangram arrangements, the matcher clicked a 'submit' button that gave players feedback on their score (out of 12) and scrambled the tangrams for the next round. After six rounds, players were redirected to a short exit survey. We collected the raw text of every message sent, and every swapping action taken by the matcher. 

## Results 

### Preprocessing 

```{r}
# Note that this includes Nicki's old data...
tagCounts = tangramMsgs %>% group_by(tangramRef) %>% tally() %>% spread(tangramRef, n)
pctTagged = 100*(1-(tagCounts$None/sum(tagCounts)))
```

Since the task was conducted with minimally constrained language use, we began by tagging which tangram was being referred to, if any, in each message. Instead of hand-tagging nearly 10,000 items, we used a hybrid strategy. First, since many of the director's messages explicitly referred to a cell number, we first cross-referenced these numbers with the locations of each tangram in their array. This successfully labelled 38.4% of utterances. Next, we combined this automatically tagged data with a previously hand-tagged pilot corpus containing 3955 utterances, and created a 80/20 split to train and evaluate a classifier that maps utterances to tangram tags. Each utterance string was vectorized into a matrix of unigram and bigram counts, then transformed into a normalized tf-idf representation (which weights counts by their overall frequency in the corpus). 

We trained our logistic classifier using stochastic gradient descent, yielding 75% accuracy on the test split. We then constructed an ROC curve examining the tradeoff between the true positive and false positive rate for different confidence thresholds, and selected a cautious threshold of 90% confidence that minimized the false positive rate to <5%. Finally, we used this threshold to label the subset of our full corpus where we are sufficiently confident. We iterated this process with batches of hand-tagging until `r round(pctTagged,2)`% of the corpus was assigned a tag.

### Reduction in raw length

First, we find that the mean number of words used by speakers decreases over time (see Fig. \ref{fig:replication}). This replicates a highly reliable reduction effect found throughout the literature on iterated reference games [@KraussWeinheimer64_ReferencePhrases;@BrennanClark96_ConceptualPactsConversation]. The following analyses, aided by modern NLP, break down this broad reduction into a finer-grained set of phenomena. 

```{r replicationfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:replication} Reduction in the mean length of utterance over time. Error bars represent 1 SE."}
ggplot(tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         summarize(m = mean(individualM), 
                   se = sd(individualM)/sqrt(length(individualM))), 
       aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("mean number words per tangram") +
  xlab("trials") +
  ylim(0,20) +
  xlim(0, 7) +
  theme_bw(8) 
```

### Part-of-speech reduction

```{r POSfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:POS} Percent reduction between first and last round for closed-class and open-class parts of speech over time. Error bars represent 1 SE."}
posReduction = read.csv('../../analysis/tangrams/posTagged.csv', header =T) %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>% 
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            #numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>% 
  # Take mean & se over participants
  summarize(diffPctM = mean(diffPct),
            diffPctSE = sd(diffPct)/sqrt(length(diffPct))) %>%
  filter(POS != "OTHER") %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps', 'adverbs'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -diffPctM) )

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$diffPctM * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$diffPctM * 100

ggplot(posReduction, aes(x = POS, y = diffPctM, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = diffPctM + diffPctSE, ymin = diffPctM - diffPctSE), width = .1)+
  theme_bw(8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

What kinds of words are most likely to be dropped as the game proceeds? We used the Stanford CoreNLP part-of-speech tagger [@Toutanova03_POStagging] to count the number of words belonging to each part of speech in each message. Fig. \ref{fig:POS} shows the percent reduction of different parts of speech from the first round to the sixth round. We find that determiners ('the', 'a', 'an') are the most likely class of words to be dropped with an `r round(detReductionRate)`% reduction rate, on average. Nouns ('dancer', 'rabbit') are the least likely class to be dropped with only an `r round(nounReductionRate)`% rate. Closed-class parts of speech are strictly more likely to be dropped than open-class parts of speech. \textcolor{red}{rdh: adverbs are a bit of both... it includes words like 'up', 'down', 'just', 'so', and 'then'}

## Listener feedback aids conventionalization

```{r turntakingfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:turntaking} Ratio of messages sent by speaker relative to listener. Error bars represent 1 SE."}
listenerMsgs <- tangramCombined %>% 
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) 

listenerMsg_lm = summary(lmer(matcher ~ roundNum + (1 | gameid), data = listenerMsgs))

ggplot(listenerMsgs %>%    
         group_by(roundNum) %>% 
         summarize(m = mean(matcher),
             se = sd(matcher)/sqrt(length(matcher))), aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("# listener messages") +
  xlab("trials") +
  #ylim(.5,1) +
  xlim(0, 7) +
  theme_bw(8) 

```

```{r}
turnTaking <- listenerMsgs %>% 
  filter(roundNum %in% c(1)) %>%
  group_by(gameid) %>%
  summarize(numListenerMsgs = mean(matcher)) %>%
  filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs)

efficiency <- tangramCombined %>% 
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>% 
   summarize(individualM = sum(numRawWords)/12) %>%
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct)

turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), aes(x = numListenerMsgs, y = diffPct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw() +
  ylab("% reduction") +
  xlab("# listener messages on 1st round")

turnTakingEfficiency_lm <- summary(lm(diffPct ~ numListenerMsgs, data = efficiency %>% left_join(turnTaking)))
turnTakingdf <- turnTakingEfficiency_lm$df[1]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), ", p = ", round(turnTakingCoefs[4],2))
```

\textcolor{red}{rdh: I don't think we want this here as-is, since our model doesn't allow listener responses, but I figured I'd put it in for the draft just in case...}

The theory proposed by @ClarkWilkesGibbs86_ReferringCollaborative argues that lexical conventions are established through a collaborative process requiring both speaker and listener input. This predicts that (1) listener feedback should be highest on the first round and drop off once meanings are agreed upon, and (2) dyads with more initial listener feedback should converge on more efficient conventions. We find both of these patterns in our data. The number of listener messages decreases significantly over the game ($t = -13.23$, see Fig. \ref{fig:turntaking}), and there is a weak but significant effect of initial listener messages on overall reduction (`r turnTakingResult`).

## Distinctive words are more likely to conventionalize

```{r PMI, fig.env = "figure", fig.pos = "b", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:pmi} Distinctiveness predicts conventionalization."}
distinctiveness_d <- read.csv("../../analysis/tangrams/matchAndPMI.csv", header = TRUE) %>%
  filter(pmi > 0) %>%
  rename(num_occurrences = total) %>%
  filter(num_occurrences > 1) %>%
  #filter(POS == "NN") %>%
  mutate(bunny = word == "bunny") %>%
  mutate(a_match = word == "a")

# summary(lm(match ~ pmi, data = distinctiveness_d))
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

ggplot(distinctiveness_d, aes(x = pmi, y = match)) +
  geom_point(aes(size = num_occurrences)) +
  geom_smooth(method = 'lm') +
  theme_bw(8) +
  scale_colour_manual(values=cbbPalette)+
  theme(
    plot.margin=unit(x=c(0,0,0,0),units="mm"),
    legend.position="top") +
  guides(color=FALSE, fill=guide_legend(title.position="top"))
```

see Fig. \ref{fig:pmi}.

## Arbitrariness and path-dependence

```{r entropy, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:entropy} High variability in referring expression across dyads; low variability within dyads."}

oldGrams = read.csv("../../analysis/tangrams/handTagged.csv", quote = '"') %>%
  mutate(numRawWords = 1 + str_count(contents, fixed(" "))) %>%
  mutate(strippedContents = str_replace_all(contents, "[^[:alnum:][:space:]']",' ')) %>%
  do(mutate(., cleanMsg = rm_stopwords(.$strippedContents, tm::stopwords("english"), 
                                       separate = F))) %>%
  mutate(numCleanWords = 1 + str_count(cleanMsg, fixed(" "))) %>%
  filter(numRawWords < mean(numRawWords) + 3*sd(numRawWords)) # Get rid of outliers

withinPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != "None") %>%
  filter(tangramRef != "*") %>%
  group_by(gameid, tangramRef) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  group_by(tangramRef) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent))) 

acrossPair <- oldGrams %>% 
  filter(tangramRef != 0) %>%
  filter(tangramRef != 'None') %>%
  filter(tangramRef != '*') %>%
  group_by(tangramRef, roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(cleanMsg, collapse = " ")))))))) %>%
  left_join(withinPair, by = "tangramRef") %>%
  gather(type, entropy, acrossEnt, withinEnt)

ggplot(acrossPair, aes(x = roundNum, y = entropy, 
                       color = type, linetype = tangramRef)) +
  geom_line() +
  theme_bw(8)
``````

see Fig. \ref{fig:entropy}

# Model

While simple evolutionary or agent-based models have previously been used to investigate the dynamics of *global* convention formation in signalling games [@ShohamTennenholtz97_EmergenceOfConventions;@Delgado02_ConventionsNetworks;@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems], relatively little modeling work has focused on the cognitive mechanisms supporting emergence of local conventions during shorter dyadic interactions. In fact, the convergence on semi-arbitrary yet efficient referring expressions within an interaction poses some problems for these classes of models. Pure evolutionary models do not provide a mechanism for agents to adapt their signaling strategy within a lifetime, let alone within an extended interaction. Emergence-through-use models, such as the one proposed by @Barr2004_ConventionalCommunicationSystems, use simple updating rules based on success or failure in an interaction, but are not easily extended to richer language models and cannot straightforwardly explain the sharp reduction in word length across a repeated interaction with a single partner \textcolor{red}{rdh: might want to actually show this if I'm going to make that argument...}

Here, we present a probabilistic model of language production under uncertainty, which captures several of the signiture properties of conventions shown above. This model belongs to the family of Rational Speech Act (RSA) models, which have been successful in explaining a wide range of linguistic phenomena -- including scalar implicature, adjectival vagueness, overinformativeness, indirect questions, and non-literal language use -- as arising from a process of recursive social reasoning. Most previous applications of RSA have focused on the listener's problem of language comprehension, but the puzzle of conventionalization is primarily a question of speaker production. An $n$th order pragmatic speaker trying to convey a particular state of affairs $s \in \mathcal{S}$ assuming lexicon $\mathcal{L}$ is assumed to select an utterance $u \in \mathcal{U}$ by trading off its expected informativity (with respect to a rational listener agent) against its cost, usually based on length [@GoodmanFrank16_RSATiCS]:

$$S_n(u | s, \mathcal{L}) \propto \exp{\alpha \log L_n(s | u, \mathcal{L}) - \textrm{cost}(u)}$$

where $\alpha$ is an optimality parameter controlling the extent to which the speaker maximizes over the expected listener distribution. The listener, in turn, reasons about what utterances would be most likely to be produced by a speaker intending to convey $u$:

$$L_n(s | u, \mathcal{L}) \propto P(s) S_{n-1}(u | s, \mathcal{L})$$

This recursion bottoms out in a *literal listener* who directly looks up the meaning of the utterance in the lexicon:

$$L_0(s | u, \mathcal{L}) \propto \mathcal{L}(u, s)\cdot P(s)$$

As in other recent applications of RSA, we take meanings as mappings from utterance-state pairs to the interval $[0,1]$. These real-valued scores represent the extent to which the utterance applies to the state, and are normalized to sum to one for a given utterance: 

Our approach to convention-formation begins with the additional assumption of *lexical uncertainty* [@SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS;@BergenLevyGoodman16_LexicalUncertainty]. In other words, we assume that instead of having perfect knowledge of $\mathcal{L}$, the speaker has uncertainty over the exact meanings of lexical items in the current context (i.e. it is initially unclear which of the ambiguous tangram shapes "the skater" might refer to) and learns these meanings through repeated interactions with a knowledgeable partner:

$$P(\mathcal{L} | \{s_i, u_i\}) = P(\mathcal{L})\prod_i P(\{s_i, u_i\}|\mathcal{L})$$

where $\{s_i, u_i\}$ is a set of observations of states $s_i$ and $u_i$ coming from previous exchanges. The speaker then marginalize over this posterior distribution when reasoning about the listener, giving rise to the form of the pragmatic listener model we use throughout our model results (only going up to $n = 1$ in our recusion for simplicity):

$$S(u | s) = \exp( \alpha\log\left(\sum_{\mathcal{L}} P(\mathcal{L} | \{s_i, u_i\}) L_1(s | u, \mathcal{L})\right) - \textrm{cost}(u) )$$

Following @SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS, we begin by showing how this speaker can converge on an arbitrary but stable naming convention despite having no *a priori* beliefs about which names correspond to which objects.

## Arbitrariness

Consider an environment with two abstract shapes ($\{s_1, s_2\}$), where the speaker must choose between two utterances ($\{u_1, u_2\}$) with equal cost. Their prior $P(\mathcal{L})$ over the meaning of each utterance is given by a Dirichlet distribution, so on the first round both utterances are equally likely to apply to either shape. The speaker, then, is equally likely to produce either utterance just by chance. 

On the second round, however, the speaker uses the listener's selection of a shape from the past round to update their posterior over lexicons. Assuming a knowledgeable listener, lexicons in which the chosen utterance corresponds to the chosen shape become more likely, so the . 

## Reduction of modifiers & conjunctions



## Reduction of hedges

Something with QUDs or signaling uncertainty, which might actually work for the above cases too

# General Discussion

A critical aspect of social conventions, and linguistic conventions in particular, is their arbitrariness: "cat" (English) and "chat" (French) are equally good ways of referring to a feline in their respective language communities. However, recent studies  emphasizing specific ways in which language is non-arbitrary [@MonaghanEtAl14_ArbitraryLanguage; @DingemanseEtAl15_IconicityLanguage; @LewisFrank16_LengthOfWordsComplexity; @BlasiEtAl16_SoundMeaningAssociation] force us to consider exactly in what sense arbitrariness is an essential property of conventions. In our model, we move to a more probabilistic notion of arbitrariness. The initial speaker distribution may not be perfectly uniform, but different values will nonetheless be sampled in different games. Because the result of this initial choice leads the speaker to update their beliefs about the lexicon in a path-dependent trajectory, this utterance becomes increasingly probable over future rounds. When observing a large number of games, as we did in our experimental analyses, we can therefore still observe substantial variability.

Noisy communication channel [@GibsonBergenPiantadosi13_RationalIntegrationNoisy;@BergenGoodman15_StrategicUseOfNoise]?. Utterances can be corrupted during transmission; listeners invert this noise model to infer likely intended utterances from their noisy percepts, and speakers take this source of noise into account when selecting what utterance to use. 

QUD model

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
