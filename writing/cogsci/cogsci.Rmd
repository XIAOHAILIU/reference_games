---
title: "Convention-formation in a communication model"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: 
  \author{{\large \bf Robert Hawkins, Michael Frank, Noah D. Goodman} \\ \texttt{\{rxdh, mcfrank, ngoodman\}@stanford.edu} \\ Department of Psychology \\ Stanford University}
 
abstract: 
    "What cognitive mechanisms support the emergence of linguistic conventions from repeated interaction? We present results from a large-scale, multi-player replication of the classic *tangrams task* which demonstrate three key empirical signatures constraining theories of convention-formation: arbitrariness, path-dependence, and reduction of utterance length over time. These results motivate a theory of convention-formation where agents, though initially uncertain about word meanings in context, assume others are using language with such knowledge. Thus, agents may learn about meanings by reasoning about a knowledgeable, informative partner; if all agents engage in such a process, they successfully coordinate their beliefs, giving rise to a conventional communication system. We formalize this theory in a computational model of language understanding as social inference and demonstrate that it produces all three signatures."
    
keywords:
    "conventions; pragmatics; communication"
    
output: cogsci2016::cogsci_paper
---


```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
library(devtools)
library(cogsci2016)
library(langcog)
```

```{r, libraries, include = FALSE}
library(ggplot2)
library(lme4)
library(entropy)
library(tm)
library(tidyr)
library(dplyr)
library(stringr)
library(knitr)
library(readr)
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(rwebppl)
library(langcog)
```

# Introduction

Just as drivers depend on shared behavioral conventions to safely navigate traffic, successful communication depends on a set of shared linguistic conventions. Speakers of different languages around the world refer to the same object in many different ways, yet when ordering a coffee in San Francisco, I can confidently use the English word "coffee" and assume that I will be understood. How do these conventions -- classically characterized by @Lewis69_Convention as arbitrary but stable solutions to recurring coordination problems -- form in the first place? 

While *global* conventions adopted and sustained throughout a large population of speakers may develop over longer time scales [@Campbell13_HistoricalLinguistics], we also effortlessly coordinate on *local* conventions -- or conceptual pacts [@BrennanClark96_ConceptualPactsConversation] -- within the span of a single dialogue. For example, when discussing possible conditions to use in an upcoming experiment, a team of collaborators might begin the meeting using long descriptions to refer to each condition but end the meeting using conventional terms like "condition A" and "condition B". Since global conventions are hypothesized to emerge through diffuse, repeated interactions of this more local kind [@GarrodDoherty94_GroupConventionsLinguistics], the cognitive mechanisms underlying convention-formation in such games are of foundational interest.


In a seminal study by @KraussWeinheimer64_ReferencePhrases, pairs of participants played a cooperative language game where they were presented with arrays of ambiguous shapes in randomized orders. The players were assigned the roles of *director* and *matcher* and allowed to talk freely. The matcher's goal was to rearrange their shapes to match the director's board, and the director's goal was to communicate useful descriptions. Over multiple rounds, descriptions were dramatically shortened: an early description like "upside-down martini glass in a wire stand," became simply "martini" by the end. Later studies [e.g. @ClarkWilkesGibbs86_ReferringCollaborative] refined this paradigm, using larger arrays of tangram-like figures and emphasizing the intricate back-and-forth process through which speakers and listeners negotiate over references. These studies revealed a number of key empirical signatures that inform theories of convention-formation. Here, we focus on three: arbitrariness, path-dependence, and the systematic reduction of utterance length over time. 

*Arbitrariness* is a classic definitional property of conventions [@Lewis69_Convention]: there must be multiple solutions that would be equally successful as long as both players "agree" (e.g. driving on the left vs. right side of the road).
<!-- "cat" (English) and "chat" (French) are equally good ways of referring to a feline in their respective language communities. -->
In the context of language games, different pairs may converge on different but equally successful referring expressions. By the final round, for example, the same tangram might be the 'dancer' to one pair and the 'skater' to another. *Path-dependence* is a dynamic where an early choice strongly influences the subsequent trajectory. For instance, a participant's use of a particular referring expression on the first round, even if not strongly prefered over other options, makes that expression much more likely on later rounds. *Reduction* is more specific to the reference game paradigm and refers to the transformation of longer, complex expressions into simpler expressions over the course of interaction, as @KraussWeinheimer64_ReferencePhrases observed. While this broad phenomenon has been replicated many times, exactly what is reduced  remains an open empirical question.

Theories of convention-formation differ primarily in the extent to which sophisticated social reasoning and common ground is required. At one extreme, agents use simple heuristic updating rules and do not need to represent or reason about other agents at all [@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems].
<!-- [@ShohamTennenholtz97_EmergenceOfConventions;@Delgado02_ConventionsNetworks;@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems].  -->
Simulations elegantly show how arbitrary signaling systems can spread and come to dominate large populations, but it is not clear how this mechanism alone could account for reduction in repeated interaction. [NDG: say why? might need to say that the heuristics considered are "rich get richer" style?] At the other extreme, agents explicitly consider their partner's beliefs and track what information is *mutual knowledge*, often formalized in a game theoretic setting [@Lewis69_Convention]. Though the mechanisms allowing conventions to emerge under such conditions have not been instantiated in a formal model to our knowledge, @WilkesGibbsClark92_CoordinatingBeliefs and others have proposed that agents engage in a collaborative process of establishing mutual knowledge.

In this paper, we argue for a theoretical position on the spectrum between these poles: conventions form when agents *assume conventions already exist*. In other words, agents believe there is a true lexicon used by other agents but are initially unsure of its identity. Through their interactions with a partner who is assumed to knowledgeable and informative, agents can learn this true lexicon, even though their partner in fact begins in the same state of ignorance. Agents thus coordinate on the same lexicon, which becomes conventional. To support this theory, we first conduct a large-scale, multi-player replication of the tangrams task, which has traditionally been limited to relatively small sample sizes in the lab. We demonstrate signatures of arbitrariness, path-dependence, and reduction which have been difficult to study at a fine-grained level due to the sparseness of existing data. Next, we formulate our theory in a computational model of communication in repeated reference games, based on recent successes capturing language understanding as social inference [@GoodmanStuhlmuller13_KnowledgeImplicature;@GoodmanFrank16_RSATiCS] and show that this model qualitatively produces all three signatures.

```{r image, fig.env = "figure", fig.pos = "tb", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:taskScreenshot} Example trial in experimental interface. Both players could freely use the chat box, and the matcher could click and drag the tangram images."}
img <- png::readPNG("figs/directorBoard.png")
grid::grid.raster(img)
```

# Replication of tangrams task

To collect a corpus of reference game dialogue that supports more detailed analyses of convention-formation, we ported the tangrams task used in @ClarkWilkesGibbs86_ReferringCollaborative to a real-time, multi-player web environment. 

## Methods

```{r}
# Import message data...
tangramMsgs = read_csv("../../analysis/tangrams/handTagged.csv") %>%
  rename(msgTime = time, 
         role = sender)

# Import survey data...
tangramSubjInfo = read.csv("../../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role'))

# Exclusion criteria
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 6))$gameid)
badGames <- union(incompleteIDs, nonNativeSpeakerIDs)

# filter & preprocess
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
```

### Participants

200 participants were recruited from Amazon's Mechanical Turk and paired into dyads to play a real-time communication game using the framework in @Hawkins15_RealTimeWebExperiments. We excluded \textcolor{red}{X} games because one or both of the participants reported being a native language different from English, and \textcolor{red}{Y} games for terminating early, leaving a corpus of `r numGames` complete games with a total of `r numUtterances` utterances.

### Stimuli

On every trial of the game, both participants were shown a $6 \times 2$ grid containing twelve tangram shapes, reproduced from @ClarkWilkesGibbs86_ReferringCollaborative. Cells were labeled with fixed numbers from one to twelve in order to help participants easily refer to locations in the grid (see Fig. \ref{fig:taskScreenshot}).

### Procedure

After passing a short quiz about task instructions, participants were randomly assigned the role of either 'director' or 'matcher' and automatically paired into virtual rooms containing a chat box and grid of stimuli. Both participants could freely use the chat box to communicate at any time. The director's tangrams were fixed in place, but the matcher could click and drag the shapes to reorder them. The director had to send messages about the locations of different tangrams on their fixed board (e.g. "#1 looks like an X", "2 is the one with the Y"); the matcher had to identify the corresponding tangram shapes and move them to the correct locations. When the players were satisfied that their boards matched, the matcher clicked a 'submit' button that gave players feedback on their score (out of 12) and scrambled the tangrams for the next round. After six rounds, players were redirected to a short exit survey. We collected the raw text of every message sent and every swapping action taken by the matcher. 

## Results 

```{r}
# Note that this includes Nicki's old data...
tagCounts = tangramMsgs %>% group_by(tangramRef) %>% tally() %>% spread(tangramRef, n)
pctTagged = 100*(1-(tagCounts$None/sum(tagCounts)))
```

<!-- \textcolor{red}{rdh: as Mike points out, I think this was a valient but ultimately doomed attempt... We should either run the sequential version of the task, pay turkers to label the rest, or stick to analyses that don't require these tags (gotta leave something for the journal paper...)} -->

<!-- Since the task was conducted with minimally constrained language use, we began by tagging which tangram was being referred to, if any, in each message. Instead of hand-tagging nearly 10,000 items, we used a hybrid strategy. First, since many of the director's messages explicitly referred to a cell number, we first cross-referenced these numbers with the locations of each tangram in their array. This successfully labelled 38.4% of utterances. Next, we combined this automatically tagged data with a previously hand-tagged pilot corpus containing 3955 utterances, and created a 80/20 split to train and evaluate a classifier that maps utterances to tangram tags. Each utterance string was vectorized into a matrix of unigram and bigram counts, then transformed into a normalized tf-idf representation (which weights counts by their overall frequency in the corpus).  -->

<!-- We trained our logistic classifier using stochastic gradient descent, yielding 75% accuracy on the test split. We then constructed an ROC curve examining the tradeoff between the true positive and false positive rate for different confidence thresholds, and selected a cautious threshold of 90% confidence that minimized the false positive rate to <5%. Finally, we used this threshold to label the subset of our full corpus where we are sufficiently confident. We iterated this process with batches of hand-tagging until `r round(pctTagged,2)`% of the corpus was assigned a tag. -->

### Arbitrariness and path-dependence

```{r entropy}
getCounts <- function(contents) {
  corpus <- Corpus(VectorSource(paste(contents, collapse = " ")))
  #cleanedCorpus <- tm_map(corpus, removeWords, stopwords('english'))
  return(colSums(as.matrix(DocumentTermMatrix(corpus))))
}

# Note that we explicitly do *not* normalize entropies, as this
# removes the contribution of a bigger vocabulary (as we would expect)
withinPair <- tangramCombined %>% 
  group_by(gameid) %>%
  summarize(alphabetSize = length(getCounts(contents)),
         ent = entropy(getCounts(contents))) %>%
  ungroup() 
  #summarize(normedEnt = mean(ent)/log(mean(alphabetSize))) %>%
  #summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent)))

acrossPair <- tangramCombined %>% 
  group_by(roundNum) %>% 
  summarize(alphabetSize = length(getCounts(contents)),
         ent = entropy(getCounts(contents))) %>%
  #summarize(normedEnt = mean(ent)/log(mean(alphabetSize))) %>%
  ungroup() 
  #summarize(acrossEnt = mean(ent))

ttest = t.test(withinPair$ent, acrossPair$ent)
ttestRes = paste0("t(", 
                  round(ttest$parameter, 2),
                  ") = ", 
                  round(ttest$statistic, 2),
                  ", p < 0.001")
```

We begin by examining signatures of *arbitrariness* and *path-dependence*. We operationalize these concepts by comparing the entropy of the word distribution *across* games against the entropy *within* games. Broadly speaking, entropy measures the predictability of a distribution: it is maximized when all elements are equally likely and declines as the distribution becomes more structured, i.e. when the probability mass is concentrated on a subset of elements:

$$H(W) = \sum_{w \in \mathcal{L}} P(w) \log P(w)$$

In the absence of arbitrariness, we would expect entropy to be low when we aggregate word frequencies across different games, since all players would adopt the single optimal way to refer to each tangram. Pure arbitrariness, on the other hand, predicts that every pair of participants should adopt different referring expressions, making the distribution relatively uniform across these different choices, and the entropy relatively high. 

While arbitrariness makes predictions for the word distribution aggregated *across* different games, path-dependence is concerned with word distributions aggregated over different rounds *within* a game. In the absence of path-dependence, we would expect entropy to be high within games, since participants would be randomly sampling arbitrary referents each round. Pure path-dependence, on the other hand, predicts that a given pair will continue using the same referring expressions, concentrating probability on a smaller set of words.

We can test both of these predictions with a single comparison between across-game entropy and mean within-game entropy. We find a mean within-game entropy of `r round(mean(withinPair$ent),2)` averaged across 67 games, which is significantly smaller than the mean across-game entropy of `r round(mean(acrossPair$ent), 2)` averaged across 6 rounds, `r ttestRes`. In other words, there is more variability in the expressions chosen across different speakers than there is across repeated interactions of a single speaker. This pattern, though rather crude and indirect, is consistent with both signatures of arbitrariness and path-dependence. 

\textcolor{red}{rdh: can someone check me here to make sure this section isn't too sketchy? Not sure how much of this detail I want to get into in the text: (1) the high across-game entropies are, of course, partially due to a larger support size N but I'm purposefully not normalizing by log(N) because we're precisely interested in the larger set of vocab... still feels like cheating a bit, though. (2) one way to partially control for this might be to exclude words that only occur a small number of times [i.e. cut off the long tail?] (3) Is there a better way of slicing the across-game entropies than by round? It feels weird to do a t-test where one sample size is just 6... Maybe I should do a permutation test instead? (4) there's also technically a small trend where across-game entropies go down as rounds progress due to reduction but I didn't think this was worth reporting: they're all much higher than the the within-game distribution...}

### Reduction in utterance length

```{r replicationfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:replication} Reduction in the mean length of utterance over time. Error bars are bootstrapped 95\\% CIs"}
lengthReduction = (tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         multi_boot_standard("individualM"))

ggplot(lengthReduction, aes(x = roundNum, y = mean)) +
  geom_line() +
  geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +
  ylab("mean # words per tangram") +
  xlab("trials") +
  ylim(0,23) +
  xlim(0, 7) +
  theme_bw(8) 
```

Next, we turn to a set of analyses examining reduction in utterance length over the course of the experiment. At the coarsest level, we find that the mean number of words used by speakers decreases over time (see Fig. \ref{fig:replication}). This decrease replicates a highly reliable reduction effect found throughout the literature on iterated reference games [@KraussWeinheimer64_ReferencePhrases;@BrennanClark96_ConceptualPactsConversation], although perhaps due to our purely textual (vs. spoken) interface, participants in our task used many fewer words overall. The following analyses break down this broad reduction into a finer-grained set of phenomena. 

```{r POSfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:POS} Percent reduction between first and last round for closed-class and open-class parts of speech over time. Error bars are bootstrapped 95\\% CIs."}
posReduction = read.csv('../../analysis/tangrams/posTagged.csv', header =T) %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>% 
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            #numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>% 
  filter(POS != "OTHER") %>%
  # Take mean & se over participants
  multi_boot_standard('diffPct') %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -mean) )

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$mean * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$mean * 100

ggplot(posReduction, aes(x = POS, y = mean, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1)+
  theme_bw(8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

What kinds of words are most likely to be dropped as the game proceeds? We used the Stanford CoreNLP part-of-speech tagger [@Toutanova03_POStagging] to count the number of words belonging to each part of speech in each message. Fig. \ref{fig:POS} shows the percent reduction of different parts of speech from the first round to the sixth round. We find that determiners ('the', 'a', 'an') are the most likely class of words to be dropped with an `r round(detReductionRate)`% reduction rate, on average. Nouns ('dancer', 'rabbit') are the least likely class to be dropped with only an `r round(nounReductionRate)`% rate. Closed-class parts of speech are strictly more likely to be dropped than open-class parts of speech. 

```{r xtable, results="asis", caption="fasdfasdf"}
unigrams <- read_csv("../../analysis/tangrams/wordCounts.csv", col_names = T) %>%
  group_by(word, POS, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>%
  select(word) %>% 
  filter(word != ',') %>%
  filter(word != '#') %>%
  head(n = 10)

bigrams <- read_csv("../../analysis/tangrams/bigramCounts.csv", col_names = T) %>%
  group_by(word, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>% 
  select(word) %>% 
  rename(bigrams = word) %>%
  head(n = 10)

topWords <- t(cbind(unigrams, bigrams))
colnames(topWords) <- NULL
rownames(topWords) <- c('unigrams', 'bigrams')
print(xtable(topWords, label = 'tab:words', caption = 'Top 10 unigrams and bigrams with the highest reduction'), 
      floating.environment = "table*", comment=F, table.placement = 'th')
```

However, examining the words most likely to be dropped (see Table \ref{tab:words}), we notice that alongside dropped articles, there are a number of words that form conjunctions ('and') and modifying clauses ('of', 'with', 'the right'). This result suggests that when utterances are reduced, it is not merely formal function words that are dropped from laziness. Initial phrases pile on multiple ambiguous but redundant modifiers and descriptors: as the game progresses and ambiguity of reference decreases, these additional meaningful units become less useful and can be dropped. We explicitly examined this hypothesis by running a dependency parser and tagging the occurence of dependent clauses and modifiers. 

\textcolor{red}{to-do: use dependency parser to count dependent clauses and modifiers...}

\textcolor{green}{ndg: looking forward to seeing this result. once it's in consider revising this section to make the different hypotheses about reduction more clear.}

### Listener feedback aids conventionalization

```{r turntakingfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:turntaking} Mean number of messages sent by listener per round. Error bars are bootstrapped 95\\% CIs."}
listenerMsgs <- tangramCombined %>% 
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) 

listenerMsg_lm = summary(lmer(matcher ~ roundNum + (1 | gameid), data = listenerMsgs))

listenerMsgIncrease = listenerMsgs %>%    
   group_by(roundNum) %>% 
   multi_boot_standard("matcher")

ggplot(listenerMsgIncrease, aes(x = roundNum, y = mean)) +
  geom_line() +
  geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +
  ylab("# listener messages") +
  xlab("trials") +
  #ylim(.5,1) +
  xlim(0, 7) +
  theme_bw(8) 

```

```{r}
turnTaking <- listenerMsgs %>% 
  filter(roundNum %in% c(1)) %>%
  group_by(gameid) %>%
  summarize(numListenerMsgs = mean(matcher)) %>%
  filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs)

efficiency <- tangramCombined %>% 
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>% 
   summarize(individualM = sum(numRawWords)/12) %>%
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct)

turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), aes(x = numListenerMsgs, y = diffPct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw(8) +
  ylab("% reduction") +
  xlab("# listener messages on 1st round")

turnTakingEfficiency_lm <- summary(lm(diffPct ~ numListenerMsgs, data = efficiency %>% left_join(turnTaking)))
turnTakingdf <- turnTakingEfficiency_lm$df[1]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), ", p = ", round(turnTakingCoefs[4],2))
```

Finally, the theory proposed by @ClarkWilkesGibbs86_ReferringCollaborative argues that lexical conventions are established through a collaborative process requiring both speaker and listener input. This predicts that (1) listener feedback should be highest on the first round and drop off once meanings are agreed upon, and (2) dyads with more initial listener feedback should converge on more efficient conventions. We find correlational evidence of both patterns in our data. The number of listener messages decreases significantly over the game ($t = -13.23$, see Fig. \ref{fig:turntaking}), and there is a weak but significant effect of initial listener messages on overall reduction (`r turnTakingResult`).

# Model

Here, we present a probabilistic model of language production under uncertainty, which captures several of the signiture properties of conventions shown above. This model belongs to the family of Rational Speech Act (RSA) models, which have been successful in explaining a wide range of linguistic phenomena -- including scalar implicature, adjectival vagueness, overinformativeness, indirect questions, and non-literal language use -- as arising from a process of recursive social reasoning. Most previous applications of RSA have focused on the listener's problem of language comprehension, but the puzzle of conventionalization is primarily a question of speaker production. An $n$th order pragmatic speaker trying to convey a particular state of affairs $s \in \mathcal{S}$ assuming lexicon $\mathcal{L}$ is assumed to select an utterance $u \in \mathcal{U}$ by trading off its expected informativity (with respect to a rational listener agent) against its cost, usually based on length [@GoodmanFrank16_RSATiCS]:

$$S_n(u | s, \mathcal{L}) \propto \exp{\left(\alpha \log L_n(s | u, \mathcal{L}) - \textrm{cost}(u)\right)}$$

where $\alpha$ is an optimality parameter controlling the extent to which the speaker maximizes over the expected listener distribution. The listener, in turn, reasons about what utterances would be most likely to be produced by a speaker intending to convey $u$:

$$L_n(s | u, \mathcal{L}) \propto P(s) S_{n-1}(u | s, \mathcal{L})$$

This recursion bottoms out in a *literal listener* who directly looks up the meaning of the utterance in the lexicon:

$$L_0(s | u, \mathcal{L}) \propto \mathcal{L}(u, s)\cdot P(s)$$

As in several other recent applications of RSA [@GrafEtAl16_BasicLevel], we use a graded semantics, where utterances are better or worse descriptions of particular referents. For instance, the utterance "dancer" may initially be expected to apply to a photorealistic image of a ballerina ($\mathcal{L}(\textrm{'dancer'}, \textrm{*ballerina*}) = 0.99$) more than an abstract image of one ($\mathcal{L}(\textrm{'dancer'}, \textrm{*abstract ballerina*}) =0.6$), but apply to both better than a non-category member like an image of a dog ($\mathcal{L}(\textrm{'dancer'}, \textrm{*dog*}) = 0.05$).

Our approach to convention-formation begins with the additional assumption of *lexical uncertainty* [@SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS;@BergenLevyGoodman16_LexicalUncertainty]. In other words, we assume that instead of having perfect knowledge of $\mathcal{L}$, the speaker has uncertainty over the exact meanings of lexical items in the current context (i.e. it is initially unclear which of the ambiguous tangram shapes "the dancer" might refer to).
\textcolor{green}{ndg: somewhere around here make clear that we're assuming a multi-timescale / dialog-centered structure to the lexicon, where there is uncertainty about how words are use in this conversation.... also worth citing mike's work on word learning through social inference somewhere?}
They begin with some prior $P(\mathcal{L})$ over meanings, which may be initially biased toward one certain meanings, and update these beliefs through repeated interactions with a knowledgeable partner. 

$$P(\mathcal{L} | d) \propto P(\mathcal{L})\prod_i L_{n-1}(s_i|u_i, \mathcal{L})$$
where $d = \{s_i, u_i\}$ is a set of observations of $s_i$ and $u_i$ coming from previous exchanges. \textcolor{red}{TODO: this notation makes the subscripts when describing the space of states/utterances confusing}. The speaker then marginalizes over this posterior distribution when reasoning about the listener, giving rise to the form of the pragmatic listener model we use throughout our model results (only going up to $n = 2$ in our recusion for simplicity):

$$S(u | s, d) \propto \exp( \alpha\log\left(\sum_{\mathcal{L}} P(\mathcal{L} | d) L_1(s | u, \mathcal{L})\right) - \textrm{cost}(u) )$$

A listener with lexical uncertainty can be defined similarity, simply swapping out $L_{n-1}$ in the lexicon posterior update with a knowledgeable speaker $S_{n-1}$:

$$L(s | u, d) \propto \sum_\mathcal{L}P(\mathcal{L}|d)L_1(s|u,\mathcal{L})$$

This model is implemented in the probabilistic programming language WebPPL [@GoodmanStuhlmuller14_DIPPL]\footnote{All results can be reproduced running our code in the browser at http://forestdb.org/models/conventions.html}. Following  @SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS, we begin by showing how a random initial choice is taken to be evidence for a particular lexicon and becomes the base for successful communication even though neither party knows its meaning at the outset.

## Results

### Arbitrariness and path-dependence

```{r model1, cache=TRUE, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelAcc} Accuracy rises as speaker and listener learn from the same data."}
res <- webppl(program_file = 'webpplModels/arbitrary.wppl',
             model_var = 'model',
             inference_opts = list(method="forward", samples=5000),
             data = list(alpha = 6, beta = 1, numSteps = 6),
             data_var = 'params',
             output_format = 'samples')
accuracyRes <- res %>% 
  mutate(sampleId = row_number()) %>%
  gather(infotype, val, -sampleId ) %>%
  separate(infotype, c('garbage', 'type', 'roundNum')) %>%
  select(-garbage) %>%
  spread(type, val) %>%
  group_by(roundNum, acc) %>%
  tally() %>%   
  mutate(prob = n / sum(n)) %>%
  ungroup() %>%
  filter(acc == TRUE) %>%
  mutate(roundNum = as.numeric(roundNum)) 
ggplot(accuracyRes, aes(x = roundNum, y = prob)) +
  ylab('accuracy') +
  geom_line() +
  theme_bw(8)
```

```{r model_arbitrariness, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelConvergence} Players are initially ambivalent between the two labels (arbitrariness). Different mappings on the first round leads to widely diverging trajectories in subsequent rounds (path-dependence)."}
accuracyRes <- res %>%
  mutate(sampleId = row_number()) %>%
  gather(infotype, val, -sampleId ) %>%
  separate(infotype, c('garbage', 'type', 'roundNum')) %>%
  select(-garbage) %>%
  spread(type, val) %>%
  unite(intendedPair, intended, utt, sep = '<->', remove = F) %>% 
  unite(realPair, utt, response, sep = '<->', remove = F) %>% 
  group_by(sampleId) %>% 
  mutate(firstPair = first(realPair)) %>%
  group_by(firstPair, roundNum, intendedPair) %>% 
  tally() %>%
  ungroup() %>%
  complete(firstPair, roundNum, intendedPair, fill = list(n = 0)) %>%  
  group_by(firstPair, roundNum) %>%
  mutate(prob = n/sum(n)) %>%
  filter(intendedPair == 't1<->label1') %>%
  filter(firstPair %in% c('label1<->t1', 'label1<->t2'))

ggplot(accuracyRes, aes(x = roundNum, y = prob, 
                        group = firstPair, linetype =firstPair, color = firstPair)) +
  ylab("S(label1 | t1) (expected)") +
  geom_line() +
  theme_bw(8) +
  theme( plot.margin=unit(x=c(0,0,0,0),units="mm"), legend.position="top")
```

Consider an environment with two abstract shapes ($\{s_1, s_2\}$), where the speaker must choose between two utterances ($\{u_1, u_2\}$) incurring equal cost. Their prior $P(\mathcal{L})$ over the meaning of each utterance is given by a (discretized) Dirichlet distribution, so on the first round both utterances are equally likely to apply to either shape. If the speaker was trying to get their partner to pick $s_1$, then, since each utterance is equally (un)informative, they would randomly sample one (say, $u_1$), and observe the listener's selection of a shape (say, $s_1$). On the next round, the speaker uses the observed pair $\{u_1, s_1\}$ to update their beliefs about the lexicon, uses these beliefs to generate a new utterance, and so on. To examine expected dynamics over multiple rounds, we enumerate over all possible trajectories our simulated speaker and listener models could produce.

We observe several important qualitative effects in our simulations. First, the fact that a knowledgeable listener responds to utterance $u$ with $s$ provides evidence for lexicons in which $u$ is a good fit for $s$, hence the likelihood of the speaker using $u$ to refer to $s$ increases on subsequent rounds (see Fig.\ref{fig:modelConvergence}). In other words, the initial symmetry between the meanings can be broken by initial random choices, leading to completely *arbitrary but stable mappings* in future rounds. Second, because the listener is also learning the lexicon from these observations under the same set of assumptions, they converge on a shared set of meanings; hence, expected *accuracy* rises on future rounds (see Fig. \ref{fig:modelAcc}). Third, because one's partner is assumed to be pragmatic, agents can also learn about *unheard* utterances: observing $\{u_1, s_1\}$ also provides evidence for lexicons in which $u_2$ is a good fit for $s_2$ by standard Gricean reasoning. Finally, *failed references* lead to conventions just as effectively as successful references: if the speaker intends $s_1$ and says $u_1$, but then the listener incorrectly picks $s_2$, the speaker will take this as evidence that $u_1$ actually means $s_2$ and use it that way on subsequent rounds.

\textcolor{green}{ndg: report the failed reference phenomenon in the behavioral results section!}

\textcolor{red}{possible TODOs: split out model curves by diff alphas (currently shown for alpha = 5)? Collapse across 'conventional systems', e.g. treat t1:label1/t2:label2 as one system and t1:label2/t2:label1 as another?}

\textcolor{green}{ndg: it'd be good to show some more gradual curves, which presumably would come from other params?}

## Reduction in utterance length

```{r modelReduction, cache=TRUE}
conjunctionRes <- webppl(program_file = 'webpplModels/conjunction.wppl',
             model_var = 'model',
             inference_opts = list(method="forward", samples=1000),
             data = list(numSteps=6, alpha=13, beta=1),
             data_var = 'params', 
             output_format = 'samples')
```

```{r modelReductionPlot, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelReduction} Utterances tend to get shorter over time"}
reductionRes <- conjunctionRes %>% 
  select(value.utt.0:value.acc.3) %>%
  mutate(sampleId = row_number()) %>%
  gather(infotype, val, -sampleId ) %>%
  separate(infotype, c('garbage', 'type', 'roundNum')) %>%
  spread(type, val) %>%
  mutate(numWords = str_count(utt, "\\S+")) %>%
  group_by(roundNum, garbage) %>%
  multi_boot_standard('numWords')

ggplot(reductionRes, aes(x = roundNum, y = mean, group = garbage)) +
  ylab('mean utterance length') +
  geom_line() +
  geom_errorbar(aes(ymax = ci_upper, ymin = ci_lower), width = .1) +  
  theme_bw(8)
```

Finally, we show that by (1) adding a small initial bias on the meanings of utterances, and (2) extending our grammar to include conjunctions of lexical items, speakers are expected to reduce the length of their utterances over multiple interactions (see \ref{fig:modelReduction}). Consider a simplified scenario in there are two objects $\{o_1, o_2\}$ each with ambiguous values on two different dimensions: shape and color. A speaker has four basic words at their disposal: two words for shape ($\{s_A, s_B\}$) and two for color $\{c_A, c_B\}$. Just as we imagine a participant who hears 'ice skater' on the first round of our tangrams task to be more likely to select some objects more than others, we suppose that our speaker's initial prior over the meaning of color and type terms to be slightly biased. $s_A$ and $c_A$ are more likely to mean $o_1$; $s_B$ and $c_B$ are more likely to mean $o_2$.

In addition to uttering these four basic words alone, we allow speakers to form conjunctions naming multiple properties, with a derived product semantics, i.e. 
$$\mathcal{L}(s_A \textrm{ and } c_A, o_1) = \mathcal{L}(s_A, o_1) \times \mathcal{L}(c_A, o_1)$$

Intuitively, if a speaker has uncertainty over the individual meanings of $s_A$ and $c_A$, it is useful to produce the conjunction to hedge against the case that one or the other basic term does not apply to the desired object. Upon observing the listener's response (say, $o_1$), the evidence is indeterminate about the separate meanings of $s_A$ and $c_A$ but both become increasingly likely to refer to $o_1$. Because of cost considerations, the shorter utterances are still assigned some probability. Once the speaker samples one of the two component utterances, the symmetry breaks and that utterance becomes the most probable in future rounds. We ran 1000 forward samples from this model and averaged over the utterance length at each round. Our results are shown in Figure \ref{fig:modelReduction}: the expected utterance length decreases systematically over each round.

\textcolor{green}{ndg: i like this subsection, but since it's the money it'd be worth trying to bring out the zing a bit: make it really clear how the model setup corresponds to tangrams and what the results mean. maybe run a simulation where the initial semantics and more certain (like the "skater" to photo of skater example earlier), to make the point that some amount of initial vagueness is needed? and discuss to hammer home the idea that reduction comes from an initial rational redundency giving over to shortened forms as reference becomes more reliable.}


# General Discussion

In this paper, we revisited the classic phenomenon of convention-formation in a large-scale replication of the tangrams task.
\textcolor{green}{ndg: remind reader about the novel empirical findings. reduction of modifiers / clauses. convention from failed reference?}
We argued that several empirical signatures including arbitrariness, path-dependence, and the reduction of utterance length over repeated interactions can be explained by our model of informative communication under lexical uncertainty. This model formalizes a theory where conventions emerge via initially uncertain agents assuming that conventions are already in place and inferring them by reasoning about a knowledgeable, informative partner.  Two concepts -- *common ground* and *arbitrariness* -- have been of particular interest to recent theoretical debates about conventions, and it's worth sketching out our model's implications for both, in turn. 

Theories of convention-formation vary in the extent to which social reasoning about common ground is required. Our agents lie on a spectrum between the non-representational agents of @Barr2004_ConventionalCommunicationSystems and the sophisticated agents of @ClarkWilkesGibbs86_ReferringCollaborative, who collaboratively build up explicit representations of mutual knowledge. 
<!-- Our agents critically make use of minimal social representations of other agents when learning the lexicon, but because they assume the other agent is fully knowledgeable, they do not use an explicit representation of common ground.  -->
Our agents implicitly coordinate by updating their beliefs based on a shared history of observations, which serves as "common ground" in an informal sense, but do not explicitly consider the fact that this history is shared when reasoning about their partner. 
\textcolor{green}{ndg: perhaps say that our model is sort of a lower bound or proof of principle for what's needed to capture lexical conventions in these games? this is alluded to in teh last paragraph.}

Recent stu  dies have also called into question the extent to which linguistic convention are, in fact, arbitrary [@DingemanseEtAl15_IconicityLanguage; @LewisFrank16_LengthOfWordsComplexity]
<!-- [@MonaghanEtAl14_ArbitraryLanguage; ; @BlasiEtAl16_SoundMeaningAssociation]  -->
This forces us to consider exactly in what sense arbitrariness is an essential property of conventions. In our model, we move to a more probabilistic notion of arbitrariness. The initial speaker distribution may not be perfectly uniform, but different values will nonetheless be sampled in different games. Because the result of this initial choice leads the speaker to update their beliefs about the lexicon in a path-dependent trajectory, this utterance becomes increasingly probable over future rounds. When observing a large number of games, as we did in our experimental analyses, we can therefore still observe substantial variability. \textcolor{green}{ndg: this paragraph is a bit weak.}

<!-- relatively little modeling work has focused on the cognitive mechanisms supporting emergence of local conventions during shorter dyadic interactions. In fact, the convergence on semi-arbitrary yet efficient referring expressions within an interaction poses some problems for these classes of models. Pure evolutionary models do not provide a mechanism for agents to adapt their signaling strategy within a lifetime, let alone within an extended interaction. Emergence-through-use models, such as the one proposed by @Barr2004_ConventionalCommunicationSystems, use simple updating rules based on success or failure in an interaction, but are not easily extended to richer language models and cannot straightforwardly explain the sharp reduction in word length across a repeated interaction with a single partner \textcolor{red}{rdh: might want to actually show this if I'm going to make that argument...} -->


By capturing reduction, which purely heuristic theories have not yet demonstrated, we showed that these minimal assumptions of social reasoning go a long way in accounting for key phenomenon. Still, our model falls short in some ways. For instance, because we do not provide a mechanisms for the listener agent to respond with confirmation, repair, or follow-up questions, we cannot make explicit predictions about the reduction in *listener messages* (as in Fig. \ref{fig:turntaking}) or the impact of early listener responses on conventionalization. These phenomena require our model to deal with planning over extended dialogues, and to potentially weaken the assumption that one's partner knows the true lexicon with complete certainty. Similarly, while our model was explicitly designed with linguistic conventions in mind, it remains to be seen whether the same formulation generalizes to broader behavioral conventions. For example, the real-time coordination games used in @HawkinsGoldstone16_SocialConventions may not require players to reason about a structured lexicon with noise, but an action policy representation may play a similar role. 

<!-- Shared conventions also allow communities of speakers to coordinate group behavior (cite collective behavior lit??) and align beliefs or memories [@StolkVerhagenToni16_ConceptualAlignment; @ComanEtAl16_MnemonicConvergence].  -->

<!-- ### Describe ways of enriching our basic model? -->
<!-- Describing vs. naming? [@Carroll80_NamingHedges] Utterances can be corrupted during transmission; listeners invert this noise model to infer likely intended utterances from their noisy percepts, and speakers take this source of noise into account when selecting what utterance to use [@BergenGoodman15_StrategicUseOfNoise] -->
<!-- [@GibsonBergenPiantadosi13_RationalIntegrationNoisy;@BergenGoodman15_StrategicUseOfNoise] -->


# Acknowledgements

We thank Nicole Maslan for her contributions during piloting. This work was supported by ONR grant N00014-13-1-0788 and a James S. McDonnell Foundation Scholar Award to NDG. RXDH was supported by the Stanford Graduate Fellowship and the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
