---
title: "Convention-formation in a communication model"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: 
  \author{{\large \bf Robert Hawkins, Michael Frank, Noah Goodman} \\ \texttt{\{rxdh, mcfrank, ngoodman\}@stanford.edu} \\ Department of Psychology \\ Stanford University}
 
abstract: 
    ""
    
keywords:
    "conventions; pragmatics; communication"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
library(devtools)
library(cogsci2016)
```

```{r, libraries, include = FALSE}
library(ggplot2)
library(lme4)
library(entropy)
library(tm)
library(tidyr)
library(dplyr)
library(stringr)
library(knitr)
library(readr)
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(rwebppl)
```

# Introduction

\textcolor{red}{rdh: Start with some standard examples of conventions?}

In a seminal study by @ClarkWilkesGibbs86_ReferringCollaborative, pairs of participants played an cooperative language game where they were presented with arrays of abstract tangram shapes in randomized orders. The players were assigned the roles of *director* and *matcher*: the matcher's goal was to rearrange their shapes to match the director's board, and the director's goal was help by communicate useful descriptions. Over six rounds, descriptions were dramatically shortened: an early description like "All right, the next one looks like a person who’s ice skating, except they’re sticking two arms out in front" became "The ice skater" by the final round. This was taken as evidence of a collaborative process by which participants coordinate on a mutually understood referring expression, or convention, that minimizes joint effort. 

Successful communication depends on a set of shared linguistic conventions: arbitrary but self-sustaining solutions to coordination problems [@Lewis69_Convention]. These conventions allow communities of speakers to coordinate group behavior (cite collective behavior lit??), initiate speech acts [@Strawson64_IntentionConvention], and align beliefs or memories [@StolkVerhagenToni16_ConceptualAlignment; @ComanEtAl16_MnemonicConvergence]. 
While *global* conventions adopted and sustained throughout a large population of speakers may form over much longer time scales (cite historical ling?), we also effortlessly coordinate on *ad hoc* or *local* conventions -- or conceptual pacts -- within the span of a single dialogue. Since global conventions emerge through repeated interactions of this kind [@GarrodDoherty94_GroupConventionsLinguistics], the cognitive mechanisms underlying conventionalization in dyadic games are of great interest.

These results raised a number of theoretical questions that remain unresolved: To what extent are these conventions arbitrary? \textcolor{red}{Why are some words from the initial utterance more likely to be dropped in future rounds than others? [mf: this particular question feels very paradigm specific - what does it represent? I think you want to motivate the work by using quantitative signatures in the data to understand the mechanisms of convention formation... sub-questions are about signatures of different theories (e.g., the words that get dropped could signal one or the other mechanism)].} If conventions emerge from a process of social reasoning, how do agent represent one another, and how do they update those representations as the game progresses? In this paper, we first use modern NLP techniques to expand upon these early results in a large-scale replication of the tangrams task. 
Next, we show that a computational model of communication in repeated reference games, based on recent successes capturing language understanding as social inference [@GoodmanStuhlmuller13_KnowledgeImplicature;@GoodmanFrank16_RSATiCS], can account for key empirical signatures of conventionalization, including arbitrariness, path-dependence, and systematic shortening of utterances over time. 

\textcolor{red}{Need a theoretical summary of modeling contribution here. One possibility is to frame it in terms of the need for pragmatics \& social reasoning (vs. pure heuristic updating as in Barr, 2004) or in terms of our perspective on establishing common ground (i.e. that you don't need to represent the common ground as an infinite recursion or some separate object you both reason about, just need to assume that other player knows true meanings...)}

<!-- We argue that the initial seeds of conventions are *non-arbitrary* to a degree, in the sense that they are chosen proportional to their informativity in context. Yet subsequent usage is also *path-dependent* in the sense that different seeds persist across time within different groups. -->

# Large-scale tangrams replication

```{r}
# Import message data...
tangramMsgs = read_csv("../../analysis/tangrams/handTagged.csv") %>%
  rename(msgTime = time, 
         role = sender)

# Import survey data...
tangramSubjInfo = read.csv("../../data/tangrams_unconstrained/turk/tangrams-subject_information.csv") %>%
  rename(gameid = gameID) %>%
  select(-workerid, -DirectorBoards, -initialMatcherBoards)

rawTangramCombined <- tangramMsgs %>% left_join(tangramSubjInfo, by = c('gameid', 'role'))

# Exclusion criteria
nonNativeSpeakerIDs <- unique((tangramSubjInfo %>% filter(nativeEnglish != "yes"))$gameid)
incompleteIDs <- unique((rawTangramCombined %>% group_by(gameid) %>% 
                           filter(length(unique(roundNum)) != 6))$gameid)
badGames <- union(incompleteIDs, nonNativeSpeakerIDs)

# filter & preprocess
tangramCombined <- rawTangramCombined %>%
  filter(!(gameid %in% badGames)) %>%
  mutate(numRawWords = str_count(contents, "\\S+")) %>%
  filter(!is.na(numRawWords)) # filter out pure punctuation messages

numGames <- length(unique(tangramCombined$gameid))
numUtterances <- length(tangramCombined$contents)
```

## Participants

200 participants were recruited from Amazon's Mechanical Turk and paired into dyads to play a real-time communication game using the framework in @Hawkins15_RealTimeWebExperiments. We excluded \textcolor{red}{X} games because one or both of the participants reported being a native language different from English, and \textcolor{red}{Y} games for terminating early, leaving a corpus of `r numGames` complete games with a total of `r numUtterances` utterances. \textcolor{red}{rdh: this technically includes nicki's data, too; need to make this clearer}

## Stimuli

```{r image, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:taskScreenshot} Example trial"}
img <- png::readPNG("figs/directorBoard.png")
grid::grid.raster(img)
```

On every trial of the game, both participants were shown a grid of twelve tangram shapes, reproduced from @ClarkWilkesGibbs86_ReferringCollaborative. The cells were all labeled with numbers from one to twelve, as an aid in reference (see Fig. \ref{fig:taskScreenshot}).

## Procedure

After passing a short quiz about task instructions, participants were randomly assigned the role of either 'director' or 'matcher' and automatically paired into virtual rooms containing a chat box and grid of stimuli. Both participants could freely use the chat box to communicate at any time, but only the matcher could click and drag stimuli to reorder them. When the players were satisfied with their tangram arrangements, the matcher clicked a 'submit' button that gave players feedback on their score (out of 12) and scrambled the tangrams for the next round. After six rounds, players were redirected to a short exit survey. We collected the raw text of every message sent, and every swapping action taken by the matcher. 

## Results 

### Preprocessing 

```{r}
# Note that this includes Nicki's old data...
tagCounts = tangramMsgs %>% group_by(tangramRef) %>% tally() %>% spread(tangramRef, n)
pctTagged = 100*(1-(tagCounts$None/sum(tagCounts)))
```

\textcolor{red}{rdh: as Mike points out, I think this was a valient but ultimately doomed attempt... We should either run the sequential version of the task, pay turkers to label the rest, or stick to analyses that don't require these tags (gotta leave something for the journal paper...)}

Since the task was conducted with minimally constrained language use, we began by tagging which tangram was being referred to, if any, in each message. Instead of hand-tagging nearly 10,000 items, we used a hybrid strategy. First, since many of the director's messages explicitly referred to a cell number, we first cross-referenced these numbers with the locations of each tangram in their array. This successfully labelled 38.4% of utterances. Next, we combined this automatically tagged data with a previously hand-tagged pilot corpus containing 3955 utterances, and created a 80/20 split to train and evaluate a classifier that maps utterances to tangram tags. Each utterance string was vectorized into a matrix of unigram and bigram counts, then transformed into a normalized tf-idf representation (which weights counts by their overall frequency in the corpus). 

We trained our logistic classifier using stochastic gradient descent, yielding 75% accuracy on the test split. We then constructed an ROC curve examining the tradeoff between the true positive and false positive rate for different confidence thresholds, and selected a cautious threshold of 90% confidence that minimized the false positive rate to <5%. Finally, we used this threshold to label the subset of our full corpus where we are sufficiently confident. We iterated this process with batches of hand-tagging until `r round(pctTagged,2)`% of the corpus was assigned a tag.

## Reduction in raw length

First, we find that the mean number of words used by speakers decreases over time (see Fig. \ref{fig:replication}). This replicates a highly reliable reduction effect found throughout the literature on iterated reference games [@KraussWeinheimer64_ReferencePhrases;@BrennanClark96_ConceptualPactsConversation]. The following analyses break down this broad reduction into a finer-grained set of phenomena. 

```{r replicationfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:replication} Reduction in the mean length of utterance over time. Error bars represent 1 SE."}
ggplot(tangramCombined %>% 
         filter(role == "director") %>%
         group_by(gameid, roundNum) %>% 
         summarize(individualM = sum(numRawWords)/12) %>% 
         group_by(roundNum) %>% 
         summarize(m = mean(individualM), 
                   se = sd(individualM)/sqrt(length(individualM))), 
       aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("mean number words per tangram") +
  xlab("trials") +
  ylim(0,20) +
  xlim(0, 7) +
  theme_bw(8) 
```

## Part-of-speech reduction

```{r POSfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:POS} Percent reduction between first and last round for closed-class and open-class parts of speech over time. Error bars represent 1 SE."}
posReduction = read.csv('../../analysis/tangrams/posTagged.csv', header =T) %>%
  # Count occurences of each POS on each round within games
  group_by(roundNum, gameid) %>% 
  summarize(numWords = sum(numWords),
            nouns = sum(nouns),
            #numbers = sum(numbers),
            verbs = sum(verbs),
            dets= sum(determiners),
            pronouns = sum(pronouns),
            preps = sum(prepositions),
            adjectives = sum(adjectives),
            adverbs = sum(adverbs)) %>%
  gather(POS, count, nouns:adverbs) %>%
  select(gameid, roundNum, POS, count) %>%
  # Need to put in ids to spread
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, count) %>%
  # Compute % reduction from first to last round
  mutate(diffPct = (round1 - round6)/round1) %>%
  group_by(POS) %>%
  # Filter out handful of people who skipped first round w/ negative %...
  filter(diffPct >= 0) %>% 
  # Take mean & se over participants
  summarize(diffPctM = mean(diffPct),
            diffPctSE = sd(diffPct)/sqrt(length(diffPct))) %>%
  filter(POS != "OTHER") %>%
  mutate(cat = ifelse(POS %in% c('dets', 'pronouns', 'preps', 'adverbs'), 'closed', 'open')) %>%
  # rearrange
  transform(POS=reorder(POS, -diffPctM) )

detReductionRate <- (posReduction %>% filter(POS == 'dets'))$diffPctM * 100
nounReductionRate <- (posReduction %>% filter(POS == 'nouns'))$diffPctM * 100

ggplot(posReduction, aes(x = POS, y = diffPctM, fill = cat)) +
  geom_bar(stat = 'identity') +
  geom_errorbar(aes(ymax = diffPctM + diffPctSE, ymin = diffPctM - diffPctSE), width = .1)+
  theme_bw(8) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("% reduction") +
  xlab("Part of Speech category")
```

What kinds of words are most likely to be dropped as the game proceeds? We used the Stanford CoreNLP part-of-speech tagger [@Toutanova03_POStagging] to count the number of words belonging to each part of speech in each message. Fig. \ref{fig:POS} shows the percent reduction of different parts of speech from the first round to the sixth round. We find that determiners ('the', 'a', 'an') are the most likely class of words to be dropped with an `r round(detReductionRate)`% reduction rate, on average. Nouns ('dancer', 'rabbit') are the least likely class to be dropped with only an `r round(nounReductionRate)`% rate. Closed-class parts of speech are strictly more likely to be dropped than open-class parts of speech. \textcolor{red}{rdh: adverbs are a bit of both... it includes words like 'up', 'down', 'just', 'so', and 'then'}

## Listener feedback aids conventionalization

```{r turntakingfig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=1.5, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:turntaking} Ratio of messages sent by speaker relative to listener. Error bars represent 1 SE."}
listenerMsgs <- tangramCombined %>% 
  group_by(gameid, roundNum, role) %>% 
  summarize(individualM = n()) %>% 
  ungroup() %>%
  complete(role, roundNum, gameid, fill = list(individualM = 0)) %>% 
  spread(role, individualM) 

listenerMsg_lm = summary(lmer(matcher ~ roundNum + (1 | gameid), data = listenerMsgs))

ggplot(listenerMsgs %>%    
         group_by(roundNum) %>% 
         summarize(m = mean(matcher),
             se = sd(matcher)/sqrt(length(matcher))), aes(x = roundNum, y = m)) +
  geom_line() +
  geom_errorbar(aes(ymax = m + se, ymin = m - se), width = .1) +
  ylab("# listener messages") +
  xlab("trials") +
  #ylim(.5,1) +
  xlim(0, 7) +
  theme_bw(8) 

```

```{r}
turnTaking <- listenerMsgs %>% 
  filter(roundNum %in% c(1)) %>%
  group_by(gameid) %>%
  summarize(numListenerMsgs = mean(matcher)) %>%
  filter(numListenerMsgs < mean(numListenerMsgs) + 3*sd(numListenerMsgs)) %>%
  select(gameid, numListenerMsgs)

efficiency <- tangramCombined %>% 
   filter(role == "director") %>%
   group_by(gameid, roundNum) %>% 
   summarize(individualM = sum(numRawWords)/12) %>%
  rowwise() %>% 
  mutate(id = row_number()) %>% 
  mutate(roundNum = paste0('round', roundNum, collapse = '')) %>%
  spread(roundNum, individualM) %>%
  mutate(diffPct = (round1 - round6)/round1) %>%
  filter(diffPct >= 0) %>% # Filter out handful of people who skipped first round...
  select(gameid, diffPct)

turnTakingEfficiencyPlot <- ggplot(turnTaking %>% left_join(efficiency), aes(x = numListenerMsgs, y = diffPct)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw() +
  ylab("% reduction") +
  xlab("# listener messages on 1st round")

turnTakingEfficiency_lm <- summary(lm(diffPct ~ numListenerMsgs, data = efficiency %>% left_join(turnTaking)))
turnTakingdf <- turnTakingEfficiency_lm$df[1]
turnTakingCoefs <- turnTakingEfficiency_lm$coefficients[2,]
turnTakingResult <- paste0("t(", turnTakingdf, ") = ", round(turnTakingCoefs[3],2), ", p = ", round(turnTakingCoefs[4],2))
```

\textcolor{red}{rdh: I don't think we want this here as-is, since our model doesn't allow listener responses, but I figured I'd put it in for the draft just in case...}

The theory proposed by @ClarkWilkesGibbs86_ReferringCollaborative argues that lexical conventions are established through a collaborative process requiring both speaker and listener input. This predicts that (1) listener feedback should be highest on the first round and drop off once meanings are agreed upon, and (2) dyads with more initial listener feedback should converge on more efficient conventions. We find both of these patterns in our data. The number of listener messages decreases significantly over the game ($t = -13.23$, see Fig. \ref{fig:turntaking}), and there is a weak but significant effect of initial listener messages on overall reduction (`r turnTakingResult`).

## Arbitrariness and path-dependence

```{r entropy, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:entropy} High variability in referring expression across dyads; low variability within dyads."}

## TODO: bootstrap errors, double-check that we're controlling for size of word distribution (right now might slightly misleading just because support of acrossPair distribution is much bigger...)
withinPair <- tangramCombined %>% 
  group_by(gameid) %>%
  summarize(ent = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(contents, collapse = " ")))))))) %>%
  summarize(withinEnt = mean(ent), withinSE = sd(ent)/sqrt(length(ent)))

acrossPair <- tangramCombined %>% 
  group_by(roundNum) %>% 
  summarize(acrossEnt = entropy(colSums(as.matrix(DocumentTermMatrix(Corpus(VectorSource(paste(contents, collapse = " "))))))))
  # left_join(withinPair, by = "tangramRef") %>%
  # gather(type, entropy, acrossEnt, withinEnt)

# ggplot(acrossPair, aes(x = roundNum, y = entropy, 
#                        color = type, linetype = tangramRef)) +
#   geom_line() +
#   theme_bw(8)
```

see Fig. \ref{fig:entropy}

# Model

While simple evolutionary or agent-based models have previously been used to investigate the dynamics of *global* convention formation in signalling games [@ShohamTennenholtz97_EmergenceOfConventions;@Delgado02_ConventionsNetworks;@Young15_EvolutionOfSocialNorms;@CentolaBaronchelli15_ConventionEmergence;@Barr2004_ConventionalCommunicationSystems], relatively little modeling work has focused on the cognitive mechanisms supporting emergence of local conventions during shorter dyadic interactions. In fact, the convergence on semi-arbitrary yet efficient referring expressions within an interaction poses some problems for these classes of models. Pure evolutionary models do not provide a mechanism for agents to adapt their signaling strategy within a lifetime, let alone within an extended interaction. Emergence-through-use models, such as the one proposed by @Barr2004_ConventionalCommunicationSystems, use simple updating rules based on success or failure in an interaction, but are not easily extended to richer language models and cannot straightforwardly explain the sharp reduction in word length across a repeated interaction with a single partner \textcolor{red}{rdh: might want to actually show this if I'm going to make that argument...}

Here, we present a probabilistic model of language production under uncertainty, which captures several of the signiture properties of conventions shown above. This model belongs to the family of Rational Speech Act (RSA) models, which have been successful in explaining a wide range of linguistic phenomena -- including scalar implicature, adjectival vagueness, overinformativeness, indirect questions, and non-literal language use -- as arising from a process of recursive social reasoning. Most previous applications of RSA have focused on the listener's problem of language comprehension, but the puzzle of conventionalization is primarily a question of speaker production. An $n$th order pragmatic speaker trying to convey a particular state of affairs $s \in \mathcal{S}$ assuming lexicon $\mathcal{L}$ is assumed to select an utterance $u \in \mathcal{U}$ by trading off its expected informativity (with respect to a rational listener agent) against its cost, usually based on length [@GoodmanFrank16_RSATiCS]:

$$S_n(u | s, \mathcal{L}) \propto \exp{\left(\alpha \log L_n(s | u, \mathcal{L}) - \textrm{cost}(u)\right)}$$

where $\alpha$ is an optimality parameter controlling the extent to which the speaker maximizes over the expected listener distribution. The listener, in turn, reasons about what utterances would be most likely to be produced by a speaker intending to convey $u$:

$$L_n(s | u, \mathcal{L}) \propto P(s) S_{n-1}(u | s, \mathcal{L})$$

This recursion bottoms out in a *literal listener* who directly looks up the meaning of the utterance in the lexicon:

$$L_0(s | u, \mathcal{L}) \propto \mathcal{L}(u, s)\cdot P(s)$$

As in several other recent applications of RSA [@GrafEtAl16_BasicLevel], we use a graded semantics, where utterances are better or worse descriptions of particular referents. For instance, the utterance "dancer" may initially be expected to apply to a photorealistic image of a ballerina ($\mathcal{L}(\textrm{'dancer'}, \textrm{*ballerina*}) = 0.99$) more than an abstract image of one ($0.6$), but apply to both better than a non-category member like an image of a dog ($0.05$).

Our approach to convention-formation begins with the additional assumption of *lexical uncertainty* [@SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS;@BergenLevyGoodman16_LexicalUncertainty]. In other words, we assume that instead of having perfect knowledge of $\mathcal{L}$, the speaker has uncertainty over the exact meanings of lexical items in the current context (i.e. it is initially unclear which of the ambiguous tangram shapes "the dancer" might refer to). They begin with some prior $P(\mathcal{L})$ over meanings, which may be initially biased toward one certain meanings, and updates these beliefs through repeated interactions with a knowledgeable partner. 

$$P(\mathcal{L} | d) = P(\mathcal{L})\prod_i L_{n-1}(s_i|u_i, \mathcal{L})$$

where $d = \{s_i, u_i\}$ is a set of observations of $s_i$ and $u_i$ coming from previous exchanges. \textcolor{red}{TODO: this notation makes the subscripts when describing the space of states/utterances confusing}. The speaker then marginalizes over this posterior distribution when reasoning about the listener, giving rise to the form of the pragmatic listener model we use throughout our model results (only going up to $n = 2$ in our recusion for simplicity):

$$S(u | s, d) = \exp( \alpha\log\left(\sum_{\mathcal{L}} P(\mathcal{L} | d) L_1(s | u, \mathcal{L})\right) - \textrm{cost}(u) )$$

A listener with lexical uncertainty can be defined similarity, simply swapping out $L_{n-1}$ in the lexicon posterior update with a knowledgeable speaker $S_{n-1}$:

$$L(s | u, d) = \sum_\mathcal{L}P(\mathcal{L}|d)L_1(s|u,\mathcal{L})$$

This model is implemented in the probabilistic programming language WebPPL [@GoodmanStuhlmuller14_DIPPL]\footnote{All results can be reproduced running our code in the browser at http://forestdb.org/models/conventions.html}. Following  @SmithGoodmanFrank13_RecursivePragmaticReasoningNIPS, we begin by showing how a random initial choice is taken to be evidence for a particular lexicon and becomes the base for successful communication even though neither party knows its meaning at the outset.

## Arbitrariness

```{r model1, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelAcc} Accuracy rises as speaker and listener learn from the same data."}
res <- webppl(program_file = 'webpplModels/arbitrary.wppl',
             model_var = 'model',
             inference_opts = list(method="enumerate"),
             data = 1,
             data_var = 'numSteps')
accuracyRes <- res %>% 
  mutate(sampleId = row_number()) %>%
  gather(infotype, val, -prob, -sampleId ) %>%
  separate(infotype, c('type', 'roundNum'), remove=TRUE, sep = '_') %>%
  spread(type, val) %>%
  group_by(roundNum, acc) %>%
  filter(acc == TRUE) %>%
  summarize(prob = sum(prob)) %>%
  ungroup() %>%
  mutate(roundNum = as.numeric(roundNum)) 
ggplot(accuracyRes, aes(x = roundNum, y = prob)) +
  ylab('accuracy') +
  geom_line() +
  theme_bw()
```

Consider an environment with two abstract shapes ($\{s_1, s_2\}$), where the speaker must choose between two utterances ($\{u_1, u_2\}$) incurring equal cost. Their prior $P(\mathcal{L})$ over the meaning of each utterance is given by a (discretized) Dirichlet distribution, so on the first round both utterances are equally likely to apply to either shape. If the speaker was trying to get their partner to pick $s_1$, then, since each utterance is equally (un)informative, they would randomly sample one (say, $u_1$), and observe the listener's selection of a shape (say, $s_1$). On the next round, the speaker uses the observed pair $\{u_1, s_1\}$ to update their beliefs about the lexicon, uses these beliefs to generate a new utterance, and so on. To examine expected dynamics over multiple rounds, we enumerate over all possible trajectories our simulated speaker and listener models could produce.

We observe several important qualitative effects in our simulations. First, the fact that a knowledgeable listener responds to utterance $u$ with $s$ provides evidence for lexicons in which $u$ is a good fit for $s$, hence the likelihood of the speaker using $u$ to refer to $s$ on subsequent rounds (see Fig.?) In other words, the initial symmetry between the meanings can be broken by initial random choices, leading to completely *arbitrary but stable mappings* in future rounds. Second, because the listener is also learning the lexicon from these observations under the same set of assumptions, they converge on a shared set of meanings; hence, expected *accuracy* rises on future rounds (see Fig. ?). Third, because one's partner is assumed to be pragmatic, agents can also learn about *unheard* utterances: observing $\{u_1, s_1\}$ also provides evidence for lexicons in which $u_2$ is a good fit for $s_2$ by standard Gricean reasoning. \textcolor{red}{rdh: maybe worth sketching out this reasoning, but it's kind of a minor point...} Finally, *failed references* lead to conventions just as effectively as successful references: if the speaker intends $s_1$ and says $u_1$, but then the listener incorrectly picks $s_2$, the speaker will take this as evidence that $u_1$ actually means $s_1$ and use it that way on subsequent rounds.

## Reduction of modifiers \& conjunctions

```{r xtable, results="asis"}
unigrams <- read_csv("../../analysis/tangrams/wordCounts.csv", col_names = T) %>%
  group_by(word, POS, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>%
  select(word) %>% 
  filter(word != ',') %>%
  filter(word != '#') %>%
  head(n = 10)

bigrams <- read_csv("../../analysis/tangrams/bigramCounts.csv", col_names = T) %>%
  group_by(word, roundNum) %>% 
  summarize(count = sum(count)) %>% 
  rowwise() %>%
  mutate(roundNum = paste0("round", roundNum, collapse = "")) %>%
  spread(roundNum, count) %>%
  filter(round1 > 10) %>%
  mutate(diffSize = round1 - round6,
         diffPct = (round1 - round6)/round1) %>% 
  arrange(desc(diffSize)) %>% 
  select(word) %>% 
  rename(bigrams = word) %>%
  head(n = 10)

topWords <- t(cbind(unigrams, bigrams))
colnames(topWords) <- NULL
rownames(topWords) <- c('unigrams', 'bigrams')
print(xtable(topWords, label = 'tab:words'), floating.environment = "table*", comment=F)
```

```{r modelResult, fig.env = "figure", fig.pos = "t", fig.align='left', fig.width=1, fig.height=1, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelResult} "}
img1 <- png::readPNG("figs/2ae683.png")
grid::grid.raster(img1)
```

```{r modelResult2, fig.env = "figure", fig.pos = "t", fig.align='right', fig.width=1, fig.height=1, set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:modelResult} todo: generate these using rwebppl"}
img2 <- png::readPNG("figs/dfc0c5.png")
grid::grid.raster(img2)
```

Examining the list of most frequently dropped unigrams and bigrams \ref{tab:words}, we see that many are informative clauses and modifiers, including the conjunction 'and'. Next, we show that by (1) adding a small initial bias on the meanings of utterances, and (2) extending our grammar to include conjunctions of lexical items, it is natural for a speaker to produce a (redundant) conjunction on the first trial and to drop one of the constituents shortly after (see \ref{fig:modelResult}).

\textcolor{red}{TODO: set-up scenario}

We see that there’s an initial preference for the longer conjunction of "type_a" and "color_a" despite the utterance cost because

(1) there's an initial bias in the lexicon prior for 'a' utterances to correspond to properties with the value 0 (thus why neither of the 'b' utterances are assigned any probability)

(2) the conjunction hedges against unlikely but possible lexicons where one or the other utterance actually corresponds to a property with value 1.

After observing an example of this conjunction referring to the first state, the conjunction actually becomes more preferred by the speaker, as it increases the probability of utterances where both type_a and color_a mean the first state. The evidence is indeterminate about the separate meanings of "type_a" and "color_a". Because of cost considerations, however, the shorter utterances are still assigned some probability. Once the speaker produces one or the other short utterances by chance, then, it breaks the symmetry and this shorter utterance becomes the most probable in future rounds.


## Reduction of hedges

Something with QUDs or signaling uncertainty, which might actually work for the above cases too

# General Discussion

A critical aspect of social conventions, and linguistic conventions in particular, is their arbitrariness: "cat" (English) and "chat" (French) are equally good ways of referring to a feline in their respective language communities. However, recent studies  emphasizing specific ways in which language is non-arbitrary [@MonaghanEtAl14_ArbitraryLanguage; @DingemanseEtAl15_IconicityLanguage; @LewisFrank16_LengthOfWordsComplexity; @BlasiEtAl16_SoundMeaningAssociation] force us to consider exactly in what sense arbitrariness is an essential property of conventions. In our model, we move to a more probabilistic notion of arbitrariness. The initial speaker distribution may not be perfectly uniform, but different values will nonetheless be sampled in different games. Because the result of this initial choice leads the speaker to update their beliefs about the lexicon in a path-dependent trajectory, this utterance becomes increasingly probable over future rounds. When observing a large number of games, as we did in our experimental analyses, we can therefore still observe substantial variability.

Is a noisy communication channel useful [@GibsonBergenPiantadosi13_RationalIntegrationNoisy;@BergenGoodman15_StrategicUseOfNoise]?. Utterances can be corrupted during transmission; listeners invert this noise model to infer likely intended utterances from their noisy percepts, and speakers take this source of noise into account when selecting what utterance to use. 

What does our model say about common ground? It's situated somewhere between the non-representational agents of @Barr2004_ConventionalCommunicationSystems and the explicit representation of a common ground memory store used by Brown-Schmidt (XXXX). Agents coordinate on conventions by updating their beliefs based on the same objective history of observations. Still, our model will not be able to account for some empirical effects, such as the impact of explicit listener feedback on the formation of efficient conventions, without relaxing the assumption that one's partner knows the true lexicon with complete certainty. Listeners often reply with question or paraphrases or suggestions for better names. This will require our RSA model to deal with extended dialogues.

While our model was explicitly designed to account for a language phenomenon, many behavioral conventions share the same properties. For example, the real-time coordination games used in @HawkinsGoldstone16_SocialConventions may not require players to reason about a structured lexicon with noise, but an action policy representation may play a similar role. 

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
