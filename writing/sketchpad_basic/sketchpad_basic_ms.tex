%% Documentclass:
\documentclass[manuscript]{stjour}

%% Manuscript, for double spaced, larger fonts
% \documentclass[manuscript]{stjour}
%% Only needed if you use `manuscript' option
\journalname{Open Mind}
%\journalname{Computational Psychiatry}


%%%%%%%%%%% Please supply information %%%%%%%%%%%%%%%%%%%%%%%%%

%% For Open Mind:
%% Supplementary Materials:
% \supplementslinks{dx.doi.org/10.1098/rsif.2013.0969}

%% For Computational Psychiatry
% \supportinginfo{dx.doi.org/10.7910/DVN/PQ6ILM}

%% If no conflicts, this command doesn't need to be used
%% \conflictsofinterest{}

%%%%%%%%%%% to be supplied by MIT Press, only %%%%%%%%%%%%%%%%%

\received{Day Month Year}
\accepted{Day Month Year}
\published{Day Month Year}

%% DOI address:
\setdoi{10.1098/rsif.2013.0969}

%%%%%%%% End MIT Press commands %%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% author definitions should be placed here:

%% example definition
\def\taupav{\tau_{\mathrm{Pav}}}

\begin{document}
\title{Contextual flexibility in visual communication}

\author[Author Names]
{Anonymous}

% \author[Author Names]
% {Judith E. Fan \affil{1,*}, Robert X.D. Hawkins \affil{1,*}, Mike Wu \affil{2}, \and Noah D. Goodman \affil{1,2}}

% \affiliation{1}{Department of Psychology, Stanford University, Stanford, CA 94305}
% \affiliation{2}{Department of Computer Science, Stanford University, Stanford, CA 94305}
% \affiliation{*}{These authors contributed equally to this work.}

% \correspondingauthor{Judith E. Fan}{jefan@stanford.edu}

\keywords{drawing, deep learning, pragmatics, computational modeling, Rational Speech Act framework}

\begin{abstract}
Communication is central to the success of our species: it allows us to learn from each other, coordinate our actions, and express otherwise hidden thoughts. Critically, human communication goes beyond language production --- humans also express their ideas in visual form. Visual communication lies at the heart of key innovations, and forms the foundation for the cultural transmission of knowledge and higher-level reasoning. This paper examines drawing, the most basic form of visual communication. Communicative uses of drawing pose a core challenge for theories of vision and communication alike: they require a detailed understanding of how sensory information is encoded and how social context guides what information is relevant to communicate. Our computational modeling approach addresses this challenge by combining a high-performing computational model of vision with a formal Bayesian model of social reasoning during communication in order to explain how people flexibly adapt their drawings according to the current context. We employ drawing-based reference games throughout our modeling and experimental work. These reference games involve two players: a \textit{sketcher} who aims to help a \textit{viewer} pick out a target object from an array of alternative, distractor objects by representing it in a drawing. We found that people exploit information in common ground with their partner to efficiently communicate, and that this contextual flexibility was well explained by our computational model. In the long run, understanding the computational basis of visual communication may shed light on the nature of human visual abstraction and the emergence of graphical conventions.

\end{abstract}

% Communication is not limited to verbal language; humans can make use of many different tools to convey meaning to a partner. While theories of communication often rely upon the existence of modality-general pragmatic reasoning mechanisms, these predictions have rarely been tested outside the verbal modality. In this experiment, we aim to test how context -- the set of potential referents in common ground -- affects visual communication: using drawings to convey the identity of rich naturalistic images. Past work in the verbal modality has shown a strong influence of speaker informativity: when the intended referent is surrounded by very similar objects, speakers send more detailed and verbose messages to help the listener distinguish it. We expect analogous results to hold in the visual modality. For instance, when referring to a car in a context containing dogs and birds, speakers should use simpler, more abstract drawings than when referring to the same object in the context of other cars.

\section*{Introduction}

Communication is central to the success of our species: it allows us to learn from each other, coordinate our actions, and express otherwise hidden thoughts. Critically, human communication goes beyond vocal production --- humans also express their ideas in visible, durable forms. From ancient etchings on cave walls to modern digital displays, visual communication lies at the heart of key human innovations (e.g., writing, mathematics, navigation), and forms the foundation for the cultural transmission of knowledge and higher-level reasoning. 

Drawing --- an activity in which a person produces visual marks that yield an image  --- is perhaps the most direct and basic form of \textit{visual production}. While drawings of objects and scenes are not identical to their physical referents, they are often just as effective for evoking their real-world counterparts in human viewers \cite[]{biederman1988surface,walther2011simple}. In the appropriate context, even a few strokes can express the identity of a face \cite[]{bergmann2013impact}, a suggested route \cite[]{agrawala2001rendering}, or an intention to act \cite[]{Galantucci:2005uh}. Conventionalized representations such as maps, graphs, and diagrams can transmit high-dimensional data and convey complex ideas by recombining relatively few geometric primitives, such as boxes, lines, arrows, ellipses \cite[]{tversky2000lines}. Despite the ubiquity and importance of such visualizations, we lack a unified computational theory of how human perception, action, and social inference are integrated to support visual communication across different contexts.

Features learned by deep convolutional neural network models (DCNNs) have recently been shown to successfully capture the abstract semantic properties of drawings of objects \cite[]{fan2015common,yamins2014performance}. However, many drawings that people use to communicate are quite selective, emphasizing information that is most relevant in the current context, and omitting irrelevant details. 

Communicative uses of drawing pose a core challenge for theories of vision and communication alike: they require a detailed understanding of how sensory information is encoded, how social context guides what information is relevant to communicate, and how extended interaction between agents shapes the visual conventions that form. We propose that the key to solving this challenge is to combine high-performing models of sensory representation from deep learning with insights from Bayesian cognitive models of social reasoning in language. This novel synthesis will provide fundamental and novel insights into the core computations that connect perception, action, and social cognition to support visual communication.

Rational speech act (RSA) models provide a probabilistic framework for deriving linguistic behavior from general principles of social cognition \cite[for a recent review]{GoodmanFrank16_RSATiCS}. These models draw on ideas and insights from decision theory, probabilistic models of cognition, bounded rationality, and linguistics, taking particular inspiration from the insights of Paul Grice (\citeyear{Grice75_LogicConversation}), who provided a philosophical framework for understanding how natural language use reflects exquisite social reasoning in context. Gricean listeners assume speakers are cooperative, choosing appropriately informative utterances to convey particular meanings. Under this assumption, listeners attempt to infer the speaker's intended communicative goal, working backwards from the form of the utterance. RSA models formalizing this core idea in terms of Bayesian inference have provided unified explanations for a variety of complex linguistic patterns, achieved good quantitative fits with experimental data, and been incorporated into artificial agents that communicate with each other to solve real-world tasks.

At the core of the RSA framework is the Gricean proposal that speakers produce utterances, $u$, that are parsimonious yet informative about the state of the world. More generally, RSA formalizes a speaker, $\mathcal{S}$, as a decision-theoretic agent who selects utterances, $u$, by (soft) maximizing their utility function, $U$, given a particular state of the world, $w$: 

\begin{equation}
\mathcal{S}(u|w) \propto e^{\alpha U(u,w)}
\end{equation}

Here, $\alpha$ is a softmax parameter. As $\alpha \rightarrow \infty$, the speaker approaches the optimal maximizing policy. 

In the simplest RSA models, the speaker's utility function, $U$, trades off the extent to which the utterance is informative to the listener and the cost of producing the utterance, $C(u)$. This notion of informativity is defined as the reduction in the listener's uncertainty, and measured using the negative surprisal of the true state of the world given an utterance:

\begin{equation}
U(u;w) = \ln L(w|u) - C(u)
\end{equation}
% \ndg{fix notation: script L? JEF: trying to use script L and S for Listener and Speaker, by analogy with script V and S for Viewer and Sketcher below.}

This speaker $\mathcal{S}$ is \emph{pragmatic} because they consider informativity in terms of a rational listener agent ($\mathcal{L}$) who updates their beliefs about the world according to the literal semantics of the language ($L$). This \emph{literal listener} is able to assess the truth value of a sentence for each possible world, thus connecting tokens of their language to features of the external world:

\begin{equation}
\mathcal{L}(w|u) \propto L(u,w)
\end{equation}

Previous work has shown that RSA models account for context sensitivity in human speakers \cite[]{GrafEtAl16_BasicLevel}. For instance, if a speaker needs to refer to a Dalmatian that appears alongside a poodle and a bear, the model predicts that this speaker will prefer the more specific utterance (``Dalmatian'') to the shorter and lower-cost basic-level utterance (``dog''). This is because the literal meaning of the word ``dog'' fits both the Dalmatian and poodle, whereas the word ``Dalmatian'' only fits the Dalmatian. Consequently, the speaker model reasons that ``Dalmatian'' is more informative. By using the word ``Dalmatian'', the speaker encourages the listener to infer the single correct world state (i.e., the target is the Dalmatian) instead of allowing two world states to seem probable, one of which is incorrect (i.e., the target is the poodle). In a context where there is only one dog, however, the speaker will prefer to say ``dog'', which is as informative as ``Dalmatian'' but is less costly to produce. 

\section*{Methods}

\subsection*{Participants}

A total of 208 unique participants, who were recruited via Amazon Mechanical Turk (AMT) and grouped into pairs, completed the experiment. 4 pairs were excluded because the sketcher in these pairs did not follow instructions, annotating their sketches with text. 

\subsection*{Stimuli}

Stimuli were 3D mesh models of objects belonging to four categories (i.e., birds, chairs, cars, dogs), containing eight objects each. Forty images of each object were produced by rendering it from a 10$^{\circ}$ viewing angle (i.e., slightly above) at a fixed distance on a gray background, each rotated by an additional 9$^{\circ}$ about the vertical axis. 

\subsection*{Task}

Drawings were collected in the context of an online, sketching-based reference game (``Guess My Sketch!''). The game involved two players: a \textit{sketcher} who aims to help a \textit{viewer} pick out a target object from an array of alternative, distractor objects by representing it in a sketch. 

On each trial, both participants were shown an array of the same four objects; however, the positions of these objects were randomized for each participant. On each trial, one of the four objects was highlighted on the sketcher's screen to designate it as the target. 

Objects were grouped into eight quartets: Four of these quartets contained objects from the same category (``close''); the other four of these quartets contained objects from different categories (``far''). Each quartet was presented four times, such that each object in the quartet served as the target exactly once.

Sketchers drew using black ink on digital canvas (pen width = 5 pixels; 500 x 500 pixels) embedded in a web browser window using Paper.js (http://paperjs.org/). Participants drew  using the mouse cursor, and were not able to delete previous strokes. Each stroke of which was rendered on the viewer's screen immediately upon the completion of each stroke. There were no restrictions on how long participants could take to make their drawings. After clicking a submit button, the viewer guessed the identity of the drawn object by clicking one of the four objects in the array. Otherwise, the viewer had no other means of communicating with the sketcher. Both participants received immediate feedback: the sketcher learned which object the viewer had clicked, and the viewer learned the identity of the target. Both participants earned bonus points for each correct response. 

\section*{Results}

% We collected stroke-by-stroke svg strings produced by the sketcher on every trial, as well as the viewer's selection among the objects. We operationalize the notion of `simplicity' through a small set of derived stroke-level measures. First, we expect the raw number of strokes per trial to be higher in the close condition than the far condition. Second, we expect the the length of the svg string to be higher in the close condition than the far condition, which should be correlated with raw number of strokes but accounts for the complexity of each stroke. Finally, we will conduct a range of model-based analyses using features extracted from modern convolutional neural networks to rigorously evaluate pragmatic and non-pragmatic accounts of communication under different assumptions about how the visual system computes similarity. 

\subsection*{Task performance}

\subsection*{Sketches}



\subsubsection*{Code availability} The code for the analyses presented in this article is publicly available in a Github repository at: XX.

\subsubsection*{Data availability} The data presented in this article are publicly available at this URL: XX.

% \acknowledgments
% This work was supported by XX. Thanks to XX, XX, and XX for helpful comments. 

% \authorcontributions 
% 

% \nocite{*}
\bibliography{references}
\bibliographystyle{apalike}
\end{document}
